{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cvxpy as cp\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRSP_SPvw includes dividends\n",
    "# CRSP_SPvwx excluding dividends\n",
    "\n",
    "# Note: Rapach, Zhou, Strauss calculate the equity premium using CRSP_SPvw (includes dividends) and lagged risk free rate\n",
    "# Note: Goyal, Welch (2008) use Index and D12 and current risk free rate (not sure if they use lagged risk free rate or \"current\")\n",
    "\n",
    "# This data list includes:\n",
    "# b/m (book to market ratio),\n",
    "# de (dividend payout ratio),\n",
    "# dfr (default return spread; used to measure corporate bond returns relative to government bond returns),\n",
    "# dfy (default yield spread; difference between BAA and AAA-rated corporate bond yields),\n",
    "# dp (dividend price ratio),\n",
    "# dy (dividend yield),\n",
    "# ep (earnings price ratio),\n",
    "# infl (inflation measured through the consumer price inded CPI),\n",
    "# ltr (long-term bond rate of returns),\n",
    "# lty (long-term bond yield),\n",
    "# ntis (net equity expansion; a measure of corporate issuing activity),\n",
    "# svar (stock variance),\n",
    "# tbl (treasury bills),\n",
    "# tms (term-spread),\n",
    "# as well as one lag of the market return\n",
    "\n",
    "\n",
    "\n",
    "COLUMNS = [\"b/m\", \"de\", \"dfr\", \"dfy\", \"dp\", \"dy\", \"ep\", \"infl\", \"ltr\", \"lty\", \"ntis\", \"svar\", \"tbl\", \"tms\", \"returns_lag_1\", \"volatility_lag_1\", \"volatility\"]\n",
    "\n",
    "\n",
    "# def load_nber(data_path, file_name):\n",
    "#     \"\"\"Used to create a data set that you can use to investigate trends around recessions and other business cycles.\"\"\"\n",
    "#     nber = pd.read_csv(os.path.join(data_path, file_name))[1:]\n",
    "#     nber[\"peak\"] = pd.to_datetime(nber[\"peak\"])\n",
    "#     nber[\"trough\"] = pd.to_datetime(nber[\"trough\"])\n",
    "#     return nber\n",
    "\n",
    "def load_data(data_path, file_name):\n",
    "    data_raw = pd.read_csv(os.path.join(data_path, file_name))\n",
    "    data_raw[\"yyyymm\"] = pd.to_datetime(data_raw[\"yyyymm\"], format='%Y%m', errors='coerce')\n",
    "    data_raw[\"Index\"] = data_raw[\"Index\"].str.replace(\",\", \"\")\n",
    "    data_raw.set_index(\"yyyymm\", inplace=True)\n",
    "    data_raw[data_raw.columns] = data_raw[data_raw.columns].astype(float)\n",
    "    data_raw.rename(columns={\"Index\":\"prices\"}, inplace=True)\n",
    "\n",
    "    # Calculate missing columns according to the explaination in m Welch and Goyal (2008)\n",
    "    data_raw[\"dfy\"] = data_raw[\"BAA\"] - data_raw[\"AAA\"]\n",
    "    data_raw[\"tms\"] = data_raw[\"lty\"] - data_raw[\"tbl\"]\n",
    "    data_raw[\"de\"] = np.log(data_raw[\"D12\"]) - np.log(data_raw[\"E12\"])\n",
    "    data_raw[\"dfr\"] = data_raw[\"corpr\"] - data_raw[\"ltr\"]\n",
    "    data_raw[\"lag_prices\"] = data_raw[\"prices\"].shift()\n",
    "    data_raw[\"dp\"] = np.log(data_raw[\"D12\"]) - np.log(data_raw[\"prices\"])\n",
    "    data_raw[\"dy\"] = np.log(data_raw[\"D12\"]) - np.log(data_raw[\"lag_prices\"])\n",
    "    data_raw[\"ep\"] = np.log(data_raw[\"E12\"])  - np.log(data_raw[\"prices\"])\n",
    "\n",
    "    # All the variables should be lagged by at least one period so we are using a predictive regression, and not an explanatory / contemporaneous regression\n",
    "    data_raw = data_raw.shift().copy()\n",
    "\n",
    "    data_raw[\"returns\"] = data_raw[\"CRSP_SPvw\"] # data_raw[\"prices\"].pct_change() # Maybe use CRSP_SPvw - Value weighted return\n",
    "    data_raw[\"returns_lag_1\"] = data_raw[\"returns\"].shift()\n",
    "\n",
    "    data_raw[\"volatility\"] = data_raw[\"returns\"].rolling(12).std()\n",
    "    data_raw[\"volatility_lag_1\"] = data_raw[\"volatility\"].shift()\n",
    "    # returns = data_raw[\"returns\"].copy()\n",
    "\n",
    "    data = data_raw[COLUMNS].dropna()\n",
    "    # returns = returns[returns.index.isin(data.index)]\n",
    "    return data # , returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b/m</th>\n",
       "      <th>de</th>\n",
       "      <th>dfr</th>\n",
       "      <th>dfy</th>\n",
       "      <th>dp</th>\n",
       "      <th>dy</th>\n",
       "      <th>ep</th>\n",
       "      <th>infl</th>\n",
       "      <th>ltr</th>\n",
       "      <th>lty</th>\n",
       "      <th>ntis</th>\n",
       "      <th>svar</th>\n",
       "      <th>tbl</th>\n",
       "      <th>tms</th>\n",
       "      <th>returns_lag_1</th>\n",
       "      <th>volatility_lag_1</th>\n",
       "      <th>volatility</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yyyymm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1927-02-01</th>\n",
       "      <td>0.443706</td>\n",
       "      <td>-0.567601</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>-2.942374</td>\n",
       "      <td>-2.963349</td>\n",
       "      <td>-2.374773</td>\n",
       "      <td>-0.011299</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.050824</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>0.034465</td>\n",
       "      <td>0.034503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927-03-01</th>\n",
       "      <td>0.428501</td>\n",
       "      <td>-0.549182</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>-2.979535</td>\n",
       "      <td>-2.932946</td>\n",
       "      <td>-2.430353</td>\n",
       "      <td>-0.005714</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.051668</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>-0.002910</td>\n",
       "      <td>0.034503</td>\n",
       "      <td>0.032960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927-04-01</th>\n",
       "      <td>0.469765</td>\n",
       "      <td>-0.531456</td>\n",
       "      <td>-0.0170</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>-2.976535</td>\n",
       "      <td>-2.970053</td>\n",
       "      <td>-2.445079</td>\n",
       "      <td>-0.005747</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.046357</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.045522</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.023652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927-05-01</th>\n",
       "      <td>0.456754</td>\n",
       "      <td>-0.512916</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>-2.984225</td>\n",
       "      <td>-2.967143</td>\n",
       "      <td>-2.471309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.050514</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.023652</td>\n",
       "      <td>0.023181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927-06-01</th>\n",
       "      <td>0.434783</td>\n",
       "      <td>-0.494518</td>\n",
       "      <td>-0.0120</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>-3.025963</td>\n",
       "      <td>-2.975058</td>\n",
       "      <td>-2.531446</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.055275</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.023181</td>\n",
       "      <td>0.026061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>0.218187</td>\n",
       "      <td>-1.083898</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>-4.160174</td>\n",
       "      <td>-4.072973</td>\n",
       "      <td>-3.076276</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>-0.006121</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>-0.081683</td>\n",
       "      <td>0.051478</td>\n",
       "      <td>0.058831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-01</th>\n",
       "      <td>0.227429</td>\n",
       "      <td>-1.068101</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>-4.110113</td>\n",
       "      <td>-4.153480</td>\n",
       "      <td>-3.042012</td>\n",
       "      <td>-0.000354</td>\n",
       "      <td>-0.0421</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>-0.009732</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.093766</td>\n",
       "      <td>0.058831</td>\n",
       "      <td>0.058788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-01</th>\n",
       "      <td>0.249478</td>\n",
       "      <td>-1.052264</td>\n",
       "      <td>0.0243</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>-4.005414</td>\n",
       "      <td>-4.103464</td>\n",
       "      <td>-2.953150</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>-0.0769</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>-0.011292</td>\n",
       "      <td>0.004934</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>-0.040305</td>\n",
       "      <td>0.058788</td>\n",
       "      <td>0.062775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-01</th>\n",
       "      <td>0.218935</td>\n",
       "      <td>-1.018245</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>-4.074095</td>\n",
       "      <td>-3.997260</td>\n",
       "      <td>-3.055849</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>-0.0139</td>\n",
       "      <td>0.0398</td>\n",
       "      <td>-0.015252</td>\n",
       "      <td>0.006504</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>-0.091495</td>\n",
       "      <td>0.062775</td>\n",
       "      <td>0.063990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>0.207182</td>\n",
       "      <td>-0.983605</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>-4.118363</td>\n",
       "      <td>-4.066005</td>\n",
       "      <td>-3.134758</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>-0.017011</td>\n",
       "      <td>0.006343</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>0.080248</td>\n",
       "      <td>0.063990</td>\n",
       "      <td>0.066729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1151 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 b/m        de     dfr     dfy        dp        dy        ep  \\\n",
       "yyyymm                                                                         \n",
       "1927-02-01  0.443706 -0.567601 -0.0019  0.0095 -2.942374 -2.963349 -2.374773   \n",
       "1927-03-01  0.428501 -0.549182 -0.0019  0.0092 -2.979535 -2.932946 -2.430353   \n",
       "1927-04-01  0.469765 -0.531456 -0.0170  0.0092 -2.976535 -2.970053 -2.445079   \n",
       "1927-05-01  0.456754 -0.512916  0.0060  0.0090 -2.984225 -2.967143 -2.471309   \n",
       "1927-06-01  0.434783 -0.494518 -0.0120  0.0093 -3.025963 -2.975058 -2.531446   \n",
       "...              ...       ...     ...     ...       ...       ...       ...   \n",
       "2022-08-01  0.218187 -1.083898  0.0054  0.0115 -4.160174 -4.072973 -3.076276   \n",
       "2022-09-01  0.227429 -1.068101  0.0128  0.0108 -4.110113 -4.153480 -3.042012   \n",
       "2022-10-01  0.249478 -1.052264  0.0243  0.0110 -4.005414 -4.103464 -2.953150   \n",
       "2022-11-01  0.218935 -1.018245  0.0036  0.0116 -4.074095 -3.997260 -3.055849   \n",
       "2022-12-01  0.207182 -0.983605  0.0250  0.0117 -4.118363 -4.066005 -3.134758   \n",
       "\n",
       "                infl     ltr     lty      ntis      svar     tbl     tms  \\\n",
       "yyyymm                                                                     \n",
       "1927-02-01 -0.011299  0.0075  0.0351  0.050824  0.000470  0.0323  0.0028   \n",
       "1927-03-01 -0.005714  0.0088  0.0347  0.051668  0.000287  0.0329  0.0018   \n",
       "1927-04-01 -0.005747  0.0253  0.0331  0.046357  0.000924  0.0320  0.0011   \n",
       "1927-05-01  0.000000 -0.0005  0.0333  0.050514  0.000603  0.0339 -0.0006   \n",
       "1927-06-01  0.005780  0.0109  0.0327  0.055275  0.000392  0.0333 -0.0006   \n",
       "...              ...     ...     ...       ...       ...     ...     ...   \n",
       "2022-08-01 -0.000118  0.0270  0.0290 -0.006121  0.003188  0.0223  0.0067   \n",
       "2022-09-01 -0.000354 -0.0421  0.0290 -0.009732  0.003349  0.0263  0.0027   \n",
       "2022-10-01  0.002151 -0.0769  0.0352 -0.011292  0.004934  0.0313  0.0039   \n",
       "2022-11-01  0.004056 -0.0139  0.0398 -0.015252  0.006504  0.0372  0.0026   \n",
       "2022-12-01 -0.001010  0.0268  0.0389 -0.017011  0.006343  0.0415 -0.0026   \n",
       "\n",
       "            returns_lag_1  volatility_lag_1  volatility  \n",
       "yyyymm                                                   \n",
       "1927-02-01       0.026047          0.034465    0.034503  \n",
       "1927-03-01      -0.002910          0.034503    0.032960  \n",
       "1927-04-01       0.045522          0.032960    0.023652  \n",
       "1927-05-01       0.007324          0.023652    0.023181  \n",
       "1927-06-01       0.013021          0.023181    0.026061  \n",
       "...                   ...               ...         ...  \n",
       "2022-08-01      -0.081683          0.051478    0.058831  \n",
       "2022-09-01       0.093766          0.058831    0.058788  \n",
       "2022-10-01      -0.040305          0.058788    0.062775  \n",
       "2022-11-01      -0.091495          0.062775    0.063990  \n",
       "2022-12-01       0.080248          0.063990    0.066729  \n",
       "\n",
       "[1151 rows x 17 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nber_data = load_nber(data_path, 'NBER_Cycle_Dates.csv') # if you want to investigate what the strategy does around recessions and other business cycles\n",
    "data_path = 'data/neural-nets/'\n",
    "data = load_data(data_path, 'Predictor_Data_Monthly_2022.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b/m</th>\n",
       "      <th>de</th>\n",
       "      <th>dfr</th>\n",
       "      <th>dfy</th>\n",
       "      <th>dp</th>\n",
       "      <th>dy</th>\n",
       "      <th>ep</th>\n",
       "      <th>infl</th>\n",
       "      <th>ltr</th>\n",
       "      <th>lty</th>\n",
       "      <th>ntis</th>\n",
       "      <th>svar</th>\n",
       "      <th>tbl</th>\n",
       "      <th>tms</th>\n",
       "      <th>returns_lag_1</th>\n",
       "      <th>volatility_lag_1</th>\n",
       "      <th>volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b/m</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.322088</td>\n",
       "      <td>-0.018084</td>\n",
       "      <td>0.470257</td>\n",
       "      <td>0.856847</td>\n",
       "      <td>0.850954</td>\n",
       "      <td>0.719893</td>\n",
       "      <td>0.079116</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.033190</td>\n",
       "      <td>0.168229</td>\n",
       "      <td>0.199485</td>\n",
       "      <td>-0.052038</td>\n",
       "      <td>-0.065958</td>\n",
       "      <td>0.344901</td>\n",
       "      <td>0.351066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>0.322088</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079265</td>\n",
       "      <td>0.532186</td>\n",
       "      <td>0.504797</td>\n",
       "      <td>0.501986</td>\n",
       "      <td>-0.207536</td>\n",
       "      <td>-0.227198</td>\n",
       "      <td>-0.014050</td>\n",
       "      <td>-0.231959</td>\n",
       "      <td>0.149246</td>\n",
       "      <td>0.354923</td>\n",
       "      <td>-0.298259</td>\n",
       "      <td>0.203421</td>\n",
       "      <td>-0.028323</td>\n",
       "      <td>0.526453</td>\n",
       "      <td>0.533547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfr</th>\n",
       "      <td>-0.018084</td>\n",
       "      <td>0.079265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>-0.019921</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>-0.084368</td>\n",
       "      <td>0.028547</td>\n",
       "      <td>-0.464957</td>\n",
       "      <td>-0.014044</td>\n",
       "      <td>0.013962</td>\n",
       "      <td>-0.134102</td>\n",
       "      <td>-0.044065</td>\n",
       "      <td>0.074012</td>\n",
       "      <td>0.105943</td>\n",
       "      <td>0.083004</td>\n",
       "      <td>0.076990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfy</th>\n",
       "      <td>0.470257</td>\n",
       "      <td>0.532186</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.405020</td>\n",
       "      <td>0.400433</td>\n",
       "      <td>0.044092</td>\n",
       "      <td>-0.237287</td>\n",
       "      <td>0.073468</td>\n",
       "      <td>0.066644</td>\n",
       "      <td>-0.179955</td>\n",
       "      <td>0.535240</td>\n",
       "      <td>-0.060598</td>\n",
       "      <td>0.288591</td>\n",
       "      <td>-0.107316</td>\n",
       "      <td>0.715070</td>\n",
       "      <td>0.736681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dp</th>\n",
       "      <td>0.856847</td>\n",
       "      <td>0.504797</td>\n",
       "      <td>-0.019921</td>\n",
       "      <td>0.405020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993531</td>\n",
       "      <td>0.739679</td>\n",
       "      <td>-0.030959</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.238844</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.042437</td>\n",
       "      <td>-0.083200</td>\n",
       "      <td>-0.065315</td>\n",
       "      <td>0.295177</td>\n",
       "      <td>0.305002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dy</th>\n",
       "      <td>0.850954</td>\n",
       "      <td>0.501986</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.400433</td>\n",
       "      <td>0.993531</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.734540</td>\n",
       "      <td>-0.029045</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.231797</td>\n",
       "      <td>0.138210</td>\n",
       "      <td>0.041285</td>\n",
       "      <td>-0.081951</td>\n",
       "      <td>-0.056706</td>\n",
       "      <td>0.296264</td>\n",
       "      <td>0.305769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ep</th>\n",
       "      <td>0.719893</td>\n",
       "      <td>-0.207536</td>\n",
       "      <td>-0.084368</td>\n",
       "      <td>0.044092</td>\n",
       "      <td>0.739679</td>\n",
       "      <td>0.734540</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142035</td>\n",
       "      <td>0.011763</td>\n",
       "      <td>0.189939</td>\n",
       "      <td>0.154311</td>\n",
       "      <td>-0.087530</td>\n",
       "      <td>0.280605</td>\n",
       "      <td>-0.252865</td>\n",
       "      <td>-0.051935</td>\n",
       "      <td>-0.075914</td>\n",
       "      <td>-0.070309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infl</th>\n",
       "      <td>0.079116</td>\n",
       "      <td>-0.227198</td>\n",
       "      <td>0.028547</td>\n",
       "      <td>-0.237287</td>\n",
       "      <td>-0.030959</td>\n",
       "      <td>-0.029045</td>\n",
       "      <td>0.142035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.102468</td>\n",
       "      <td>0.215382</td>\n",
       "      <td>-0.055981</td>\n",
       "      <td>-0.205088</td>\n",
       "      <td>0.247152</td>\n",
       "      <td>-0.118209</td>\n",
       "      <td>0.038014</td>\n",
       "      <td>-0.195955</td>\n",
       "      <td>-0.206446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ltr</th>\n",
       "      <td>0.009898</td>\n",
       "      <td>-0.014050</td>\n",
       "      <td>-0.464957</td>\n",
       "      <td>0.073468</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.009101</td>\n",
       "      <td>0.011763</td>\n",
       "      <td>-0.102468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056491</td>\n",
       "      <td>-0.046603</td>\n",
       "      <td>0.092946</td>\n",
       "      <td>0.054069</td>\n",
       "      <td>-0.005492</td>\n",
       "      <td>-0.079538</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>0.015227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lty</th>\n",
       "      <td>0.193800</td>\n",
       "      <td>-0.231959</td>\n",
       "      <td>-0.014044</td>\n",
       "      <td>0.066644</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.007356</td>\n",
       "      <td>0.189939</td>\n",
       "      <td>0.215382</td>\n",
       "      <td>0.056491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062099</td>\n",
       "      <td>-0.105096</td>\n",
       "      <td>0.906857</td>\n",
       "      <td>0.022038</td>\n",
       "      <td>-0.005007</td>\n",
       "      <td>-0.118301</td>\n",
       "      <td>-0.117095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ntis</th>\n",
       "      <td>0.033190</td>\n",
       "      <td>0.149246</td>\n",
       "      <td>0.013962</td>\n",
       "      <td>-0.179955</td>\n",
       "      <td>0.238844</td>\n",
       "      <td>0.231797</td>\n",
       "      <td>0.154311</td>\n",
       "      <td>-0.055981</td>\n",
       "      <td>-0.046603</td>\n",
       "      <td>-0.062099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.038847</td>\n",
       "      <td>0.032644</td>\n",
       "      <td>-0.212402</td>\n",
       "      <td>-0.044960</td>\n",
       "      <td>-0.021977</td>\n",
       "      <td>-0.018859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svar</th>\n",
       "      <td>0.168229</td>\n",
       "      <td>0.354923</td>\n",
       "      <td>-0.134102</td>\n",
       "      <td>0.535240</td>\n",
       "      <td>0.166924</td>\n",
       "      <td>0.138210</td>\n",
       "      <td>-0.087530</td>\n",
       "      <td>-0.205088</td>\n",
       "      <td>0.092946</td>\n",
       "      <td>-0.105096</td>\n",
       "      <td>-0.038847</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.151947</td>\n",
       "      <td>0.132048</td>\n",
       "      <td>-0.214121</td>\n",
       "      <td>0.504109</td>\n",
       "      <td>0.565680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tbl</th>\n",
       "      <td>0.199485</td>\n",
       "      <td>-0.298259</td>\n",
       "      <td>-0.044065</td>\n",
       "      <td>-0.060598</td>\n",
       "      <td>0.042437</td>\n",
       "      <td>0.041285</td>\n",
       "      <td>0.280605</td>\n",
       "      <td>0.247152</td>\n",
       "      <td>0.054069</td>\n",
       "      <td>0.906857</td>\n",
       "      <td>0.032644</td>\n",
       "      <td>-0.151947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.401352</td>\n",
       "      <td>-0.015537</td>\n",
       "      <td>-0.200835</td>\n",
       "      <td>-0.194619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tms</th>\n",
       "      <td>-0.052038</td>\n",
       "      <td>0.203421</td>\n",
       "      <td>0.074012</td>\n",
       "      <td>0.288591</td>\n",
       "      <td>-0.083200</td>\n",
       "      <td>-0.081951</td>\n",
       "      <td>-0.252865</td>\n",
       "      <td>-0.118209</td>\n",
       "      <td>-0.005492</td>\n",
       "      <td>0.022038</td>\n",
       "      <td>-0.212402</td>\n",
       "      <td>0.132048</td>\n",
       "      <td>-0.401352</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025976</td>\n",
       "      <td>0.219325</td>\n",
       "      <td>0.207199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>returns_lag_1</th>\n",
       "      <td>-0.065958</td>\n",
       "      <td>-0.028323</td>\n",
       "      <td>0.105943</td>\n",
       "      <td>-0.107316</td>\n",
       "      <td>-0.065315</td>\n",
       "      <td>-0.056706</td>\n",
       "      <td>-0.051935</td>\n",
       "      <td>0.038014</td>\n",
       "      <td>-0.079538</td>\n",
       "      <td>-0.005007</td>\n",
       "      <td>-0.044960</td>\n",
       "      <td>-0.214121</td>\n",
       "      <td>-0.015537</td>\n",
       "      <td>0.025976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>0.020912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatility_lag_1</th>\n",
       "      <td>0.344901</td>\n",
       "      <td>0.526453</td>\n",
       "      <td>0.083004</td>\n",
       "      <td>0.715070</td>\n",
       "      <td>0.295177</td>\n",
       "      <td>0.296264</td>\n",
       "      <td>-0.075914</td>\n",
       "      <td>-0.195955</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>-0.118301</td>\n",
       "      <td>-0.021977</td>\n",
       "      <td>0.504109</td>\n",
       "      <td>-0.200835</td>\n",
       "      <td>0.219325</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatility</th>\n",
       "      <td>0.351066</td>\n",
       "      <td>0.533547</td>\n",
       "      <td>0.076990</td>\n",
       "      <td>0.736681</td>\n",
       "      <td>0.305002</td>\n",
       "      <td>0.305769</td>\n",
       "      <td>-0.070309</td>\n",
       "      <td>-0.206446</td>\n",
       "      <td>0.015227</td>\n",
       "      <td>-0.117095</td>\n",
       "      <td>-0.018859</td>\n",
       "      <td>0.565680</td>\n",
       "      <td>-0.194619</td>\n",
       "      <td>0.207199</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>0.978363</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       b/m        de       dfr       dfy        dp        dy  \\\n",
       "b/m               1.000000  0.322088 -0.018084  0.470257  0.856847  0.850954   \n",
       "de                0.322088  1.000000  0.079265  0.532186  0.504797  0.501986   \n",
       "dfr              -0.018084  0.079265  1.000000  0.020098 -0.019921  0.000090   \n",
       "dfy               0.470257  0.532186  0.020098  1.000000  0.405020  0.400433   \n",
       "dp                0.856847  0.504797 -0.019921  0.405020  1.000000  0.993531   \n",
       "dy                0.850954  0.501986  0.000090  0.400433  0.993531  1.000000   \n",
       "ep                0.719893 -0.207536 -0.084368  0.044092  0.739679  0.734540   \n",
       "infl              0.079116 -0.227198  0.028547 -0.237287 -0.030959 -0.029045   \n",
       "ltr               0.009898 -0.014050 -0.464957  0.073468  0.000715  0.009101   \n",
       "lty               0.193800 -0.231959 -0.014044  0.066644  0.008039  0.007356   \n",
       "ntis              0.033190  0.149246  0.013962 -0.179955  0.238844  0.231797   \n",
       "svar              0.168229  0.354923 -0.134102  0.535240  0.166924  0.138210   \n",
       "tbl               0.199485 -0.298259 -0.044065 -0.060598  0.042437  0.041285   \n",
       "tms              -0.052038  0.203421  0.074012  0.288591 -0.083200 -0.081951   \n",
       "returns_lag_1    -0.065958 -0.028323  0.105943 -0.107316 -0.065315 -0.056706   \n",
       "volatility_lag_1  0.344901  0.526453  0.083004  0.715070  0.295177  0.296264   \n",
       "volatility        0.351066  0.533547  0.076990  0.736681  0.305002  0.305769   \n",
       "\n",
       "                        ep      infl       ltr       lty      ntis      svar  \\\n",
       "b/m               0.719893  0.079116  0.009898  0.193800  0.033190  0.168229   \n",
       "de               -0.207536 -0.227198 -0.014050 -0.231959  0.149246  0.354923   \n",
       "dfr              -0.084368  0.028547 -0.464957 -0.014044  0.013962 -0.134102   \n",
       "dfy               0.044092 -0.237287  0.073468  0.066644 -0.179955  0.535240   \n",
       "dp                0.739679 -0.030959  0.000715  0.008039  0.238844  0.166924   \n",
       "dy                0.734540 -0.029045  0.009101  0.007356  0.231797  0.138210   \n",
       "ep                1.000000  0.142035  0.011763  0.189939  0.154311 -0.087530   \n",
       "infl              0.142035  1.000000 -0.102468  0.215382 -0.055981 -0.205088   \n",
       "ltr               0.011763 -0.102468  1.000000  0.056491 -0.046603  0.092946   \n",
       "lty               0.189939  0.215382  0.056491  1.000000 -0.062099 -0.105096   \n",
       "ntis              0.154311 -0.055981 -0.046603 -0.062099  1.000000 -0.038847   \n",
       "svar             -0.087530 -0.205088  0.092946 -0.105096 -0.038847  1.000000   \n",
       "tbl               0.280605  0.247152  0.054069  0.906857  0.032644 -0.151947   \n",
       "tms              -0.252865 -0.118209 -0.005492  0.022038 -0.212402  0.132048   \n",
       "returns_lag_1    -0.051935  0.038014 -0.079538 -0.005007 -0.044960 -0.214121   \n",
       "volatility_lag_1 -0.075914 -0.195955  0.007641 -0.118301 -0.021977  0.504109   \n",
       "volatility       -0.070309 -0.206446  0.015227 -0.117095 -0.018859  0.565680   \n",
       "\n",
       "                       tbl       tms  returns_lag_1  volatility_lag_1  \\\n",
       "b/m               0.199485 -0.052038      -0.065958          0.344901   \n",
       "de               -0.298259  0.203421      -0.028323          0.526453   \n",
       "dfr              -0.044065  0.074012       0.105943          0.083004   \n",
       "dfy              -0.060598  0.288591      -0.107316          0.715070   \n",
       "dp                0.042437 -0.083200      -0.065315          0.295177   \n",
       "dy                0.041285 -0.081951      -0.056706          0.296264   \n",
       "ep                0.280605 -0.252865      -0.051935         -0.075914   \n",
       "infl              0.247152 -0.118209       0.038014         -0.195955   \n",
       "ltr               0.054069 -0.005492      -0.079538          0.007641   \n",
       "lty               0.906857  0.022038      -0.005007         -0.118301   \n",
       "ntis              0.032644 -0.212402      -0.044960         -0.021977   \n",
       "svar             -0.151947  0.132048      -0.214121          0.504109   \n",
       "tbl               1.000000 -0.401352      -0.015537         -0.200835   \n",
       "tms              -0.401352  1.000000       0.025976          0.219325   \n",
       "returns_lag_1    -0.015537  0.025976       1.000000          0.040890   \n",
       "volatility_lag_1 -0.200835  0.219325       0.040890          1.000000   \n",
       "volatility       -0.194619  0.207199       0.020912          0.978363   \n",
       "\n",
       "                  volatility  \n",
       "b/m                 0.351066  \n",
       "de                  0.533547  \n",
       "dfr                 0.076990  \n",
       "dfy                 0.736681  \n",
       "dp                  0.305002  \n",
       "dy                  0.305769  \n",
       "ep                 -0.070309  \n",
       "infl               -0.206446  \n",
       "ltr                 0.015227  \n",
       "lty                -0.117095  \n",
       "ntis               -0.018859  \n",
       "svar                0.565680  \n",
       "tbl                -0.194619  \n",
       "tms                 0.207199  \n",
       "returns_lag_1       0.020912  \n",
       "volatility_lag_1    0.978363  \n",
       "volatility          1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>volatility</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatility_lag_1</th>\n",
       "      <td>0.978363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfy</th>\n",
       "      <td>0.736681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svar</th>\n",
       "      <td>0.565680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>0.533547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b/m</th>\n",
       "      <td>0.351066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dy</th>\n",
       "      <td>0.305769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dp</th>\n",
       "      <td>0.305002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tms</th>\n",
       "      <td>0.207199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infl</th>\n",
       "      <td>0.206446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tbl</th>\n",
       "      <td>0.194619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lty</th>\n",
       "      <td>0.117095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dfr</th>\n",
       "      <td>0.076990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ep</th>\n",
       "      <td>0.070309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>returns_lag_1</th>\n",
       "      <td>0.020912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ntis</th>\n",
       "      <td>0.018859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ltr</th>\n",
       "      <td>0.015227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  volatility\n",
       "volatility          1.000000\n",
       "volatility_lag_1    0.978363\n",
       "dfy                 0.736681\n",
       "svar                0.565680\n",
       "de                  0.533547\n",
       "b/m                 0.351066\n",
       "dy                  0.305769\n",
       "dp                  0.305002\n",
       "tms                 0.207199\n",
       "infl                0.206446\n",
       "tbl                 0.194619\n",
       "lty                 0.117095\n",
       "dfr                 0.076990\n",
       "ep                  0.070309\n",
       "returns_lag_1       0.020912\n",
       "ntis                0.018859\n",
       "ltr                 0.015227"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(abs(data.corr()[['volatility']])).sort_values(by='volatility', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='yyyymm'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGwCAYAAACKOz5MAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAACO8ElEQVR4nO2dd5wTZf7HPymbbF9YFnaBXZbeRDpIkaJ0K2cBy4ENy6l3Inenx9m9ot5ZwHqWE9SfFBvqWUBQkSrSFkHpbSm7LGV7yW6S5/dH8kyemcxkUybZJPt9v1772mQymcyTTPk832pgjDEQBEEQBEFEMcam3gGCIAiCIIjGIMFCEARBEETUQ4KFIAiCIIiohwQLQRAEQRBRDwkWgiAIgiCiHhIsBEEQBEFEPSRYCIIgCIKIesxNvQN64XQ6cfLkSaSlpcFgMDT17hAEQRAE4QeMMVRWVqJdu3YwGrXtKHEjWE6ePIm8vLym3g2CIAiCIILg2LFjyM3N1Xw9bgRLWloaANeA09PTm3hvCIIgCILwh4qKCuTl5Un3cS3iRrBwN1B6ejoJFoIgCIKIMRoL56CgW4IgCIIgoh4SLARBEARBRD0kWAiCIAiCiHriJoaFIAiCaD44nU7U19c39W4QfpCQkACTyRTydkiwEARBEDFFfX09Dh8+DKfT2dS7QvhJixYtkJOTE1KdNBIsBEEQRMzAGENRURFMJhPy8vJ8Fhojmh7GGGpqalBSUgIAaNu2bdDbIsFCEARBxAx2ux01NTVo164dkpOTm3p3CD9ISkoCAJSUlKBNmzZBu4dImhIEQRAxg8PhAABYLJYm3hMiELi4bGhoCHobJFgIgiCImIN6xsUWevxeJFgIgiAIgoh6SLAQBEEQBBH1kGAhCIIgiBjAYDDg008/1XU7R44cgcFgQEFBAQBg9erVMBgMKCsrC/lz9IYEi044nAw2u6Opd4MgCIIgAACPP/44+vfv77W8qKgIU6ZMUX3PiBEjUFRUhIyMDADAwoUL0aJFizDupf+QYNGJKfPXYNDfVqGugUQLQRAEEb3k5OTAarWqvmaxWEIu8BYuSLDoxL5TVaiy2fHLyfKm3hWCIIhmA2MMNfX2JvljjPm9n6+//jrat2/vVZ33iiuuwE033QQAeO2119ClSxdYLBb06NED7733ns9tPvjgg+jevTuSk5PRuXNnPPLII1La8MKFC/HEE09gx44dMBgMMBgMWLhwIQDfriXRJbR69WrccsstKC8vl7bx+OOP48knn8T555/v9d5Bgwbh0Ucf9fs7CRQqHKcDgRy0BEEQhH7UNjjQ+9EVTfLZvz45CckW/26j1157Lf7whz/g+++/x7hx4wAApaWlWLFiBf73v/9h2bJluO+++zBv3jyMHz8eX3zxBW655Rbk5ubioosuUt1mWloaFi5ciHbt2mHnzp24/fbbkZaWhgceeADTp0/Hrl27sHz5cqxatQoAJDePv4wYMQLz5s3Do48+ir179wIAUlNTUVZWhieeeAKbN2/GkCFDAAA///wztm/fjg8//DCgzwgEEiw64CS9QhAEQfggMzMTkydPxqJFiyTB8uGHHyIzMxPjxo3D6NGjcfPNN+Puu+8GAMyZMwc//vgjnn32WU3B8vDDD0uPO3bsiD/+8Y9YunQpHnjgASQlJSE1NRVmsxk5OTlB7bPFYkFGRgYMBoNsG6mpqZg0aRIWLFggCZYFCxZgzJgx6Ny5c1Cf5Q8kWHTATg24CIIgmoSkBBN+fXJSk312INx4442444478Oqrr8JqteL999/HddddB5PJhN27d+OOO+6QrT9y5EjMnz9fc3sfffQR5s2bhwMHDqCqqgp2ux3p6elBjSVQbr/9dtx66614/vnnYTKZ8P777+O5554L62eSYAmBugYHEhNMcAgmFvIOEQRBRA6DweC3W6apufzyy+F0OvHll19iyJAhWLt2LZ5//nnpdWWgK2NMM/j1xx9/xHXXXYcnnngCkyZNQkZGBpYsWRJ20cC5/PLLYbVasWzZMlitVthsNlx99dVh/czY+JWjkPUHzuDGtzbhvnHdMGtUp6beHYIgCCLKSUpKwlVXXYX3338fBw4cQPfu3TFo0CAAQK9evbBu3TrMnDlTWn/Dhg3o1auX6rbWr1+P/Px8PPTQQ9Kyo0ePytaxWCxS76Vg0dqG2WzGTTfdhAULFsBqteK6664LezNKEixB8shnuwAA87/dj1tGdmzanSEIgiBightvvBGXX345fvnlF/z2t7+Vlv/5z3/GtGnTMHDgQIwbNw7/+9//8Mknn0gBs0q6du2KwsJCLFmyBEOGDMGXX36JZcuWydbp2LEjDh8+jIKCAuTm5iItLU0znVmLjh07oqqqCt9++y369euH5ORkSZjMmjVLElTr168PaLvBQGnNQWISzHR20SXUFDtDEARBxAQXX3wxMjMzsXfvXtxwww3S8qlTp2L+/Pn497//jfPOOw+vv/46FixYgLFjx6pu58orr8T999+Pe++9F/3798eGDRvwyCOPyNa5+uqrMXnyZFx00UVo3bo1Fi9eHPD+jhgxAnfddRemT5+O1q1b41//+pf0Wrdu3TBixAj06NEDF1xwQcDbDhQDi5Oc3IqKCmRkZKC8vDwiQUeTXliDvacqAQCb/joOF/zzWwDAkjuGYVjnVmH/fIIgiOZIXV0dDh8+jE6dOiExMbGpd6dZwxhDz549ceedd2LOnDk+1/X1u/l7/yaXUJCIcVBi0K2DcpwJgiCIOKekpATvvfceTpw4gVtuuSUin0mCJUhMRo9iEUWKnQQLQRAEEedkZ2cjKysLb7zxBlq2bBmRzyTBEiRGjRgWB9VkIQiCIOKcpogmoaDbIDFqWVgcZGEhCIIIN3ESftls0OP3IsESJCaKYSEIgog4JpOrumx9fX0T7wkRCDU1NQCAhISEoLdBLqEgkbuEnMJjEiwEQRDhwmw2Izk5GadPn0ZCQgKMRpp3RzOMMdTU1KCkpAQtWrSQBGcwkGAJElGwkIWFIAgiMhgMBrRt2xaHDx/2quxKRC8tWrQIugkjhwRLkIiinrKECIIgIofFYkG3bt3ILRQjJCQkhGRZ4ZBgCRJtCwtlCREEQYQbo9FIheOaGeT8CxKxDoudLCwEQRAEEVZIsASJaGFxUgwLQRAEQYQVEixBIhhY5BYWqsNCEARBELpDgiVIKEuIIAiCICIHCZYgMVIMC0EQBEFEDBIsQWLUrHRLWUIEQRAEoTckWIJEdAl9ubNIekwWFoIgCILQHxIsQSK6hP6346T0mGJYCIIgCEJ/SLAEiUmwsIiQhYUgCIIg9IcES5AY1fUKKmobIrsjBEEQBNEMIMESJEYNxbK/pCrCe0IQBEEQ8Q8JliAxariE9p2qjPCeEARBEET8Q4IlSLRcQpV19sjuCEEQBEE0A0iwBImWhcXhZGCMAm8JgiAIQk9IsASJhl4BQJlCBEEQBKE3JFiCxJcRhRogEgRBEIS+kGAJEqcPxWKn8vwEQRAEoSskWIJEqVeyUq3SY7KwEARBEIS+kGAJEh6mcu2gXAzrnIk/jOsqxbU0kIWFIAiCIHQlKMHy6quvolOnTkhMTMSgQYOwdu1azXU/+eQTTJgwAa1bt0Z6ejqGDx+OFStWeK338ccfo3fv3rBarejduzeWLVsWzK5FDJ4J1C07FUvuGI6Zwzsiwej6OqmfEEEQBEHoS8CCZenSpZg9ezYeeughbN++HaNGjcKUKVNQWFiouv6aNWswYcIEfPXVV9i6dSsuuugiXH755di+fbu0zsaNGzF9+nTMmDEDO3bswIwZMzBt2jRs2rQp+JGFGS5JxPRmk7s4C7mECIIgCEJfDCzAoiEXXHABBg4ciNdee01a1qtXL0ydOhVPPfWUX9s477zzMH36dDz66KMAgOnTp6OiogJff/21tM7kyZPRsmVLLF682K9tVlRUICMjA+Xl5UhPTw9gRMFx35Lt+KzgJB65rDduu7ATAOD8x1egss6O7/80Fp2yUsK+DwRBEAQR6/h7/w7IwlJfX4+tW7di4sSJsuUTJ07Ehg0b/NqG0+lEZWUlMjMzpWUbN2702uakSZN8btNms6GiokL2F0m410csx5Jgcn2ddgfFsBAEQRCEngQkWM6cOQOHw4Hs7GzZ8uzsbBQXF/u1jeeeew7V1dWYNm2atKy4uDjgbT711FPIyMiQ/vLy8gIYSehww5RYop+7hBrIJUQQBEEQuhJU0K1BUeaVMea1TI3Fixfj8ccfx9KlS9GmTZuQtjl37lyUl5dLf8eOHQtgBKHDHWli1+YE92MKuiUIgiAIfTEHsnJWVhZMJpOX5aOkpMTLQqJk6dKluO222/Dhhx9i/PjxstdycnIC3qbVaoXVatV8PdzwwnGipDKZ3BYWSmsmCIIgCF0JyMJisVgwaNAgrFy5UrZ85cqVGDFihOb7Fi9ejJtvvhmLFi3CpZde6vX68OHDvbb5zTff+NxmU8MtLKIViNKaCYIgCCI8BGRhAYA5c+ZgxowZGDx4MIYPH4433ngDhYWFuOuuuwC4XDUnTpzAu+++C8AlVmbOnIn58+dj2LBhkiUlKSkJGRkZAID77rsPo0ePxjPPPIMrr7wSn332GVatWoV169bpNU7dcUoxLB7BYuYWFgq6JQiCIAhdCTiGZfr06Zg3bx6efPJJ9O/fH2vWrMFXX32F/Px8AEBRUZGsJsvrr78Ou92Oe+65B23btpX+7rvvPmmdESNGYMmSJViwYAH69u2LhQsXYunSpbjgggt0GGJ4kLKEZEG3PEuILCwEQRAEoScBW1gA4O6778bdd9+t+trChQtlz1evXu3XNq+55hpcc801wexOE+GdJZRgoqBbgiAIgggH1EsoSJwqMSyetGZyCREEQRCEnpBgCRK1LCEedGsnCwtBEARB6AoJliCR6rCo9RIiwUIQBEEQukKCJUikLCHhG+RZQlSanyAIgiD0hQRLkEh1WASnkNRLiCwsBEEQBKErJFiChLmzhAwqvYQorZkgCIIg9IUES5Dw6vuywnG8lxAjwUIQBEEQekKCJUikLCHBwsLFCyPBQhAEQRC6QoIlSLgkES0s/CEVjiMIgiAIfSHBEiSMeVe65TEspFcIgiAIQl9IsASJR5R4FAu5hAiCIAgiPJBgCRI1Cwu5hAiCIAgiPJBgCRKnWqVbA7mECIIgCCIckGAJEuYjS8hJLiGCIAiC0BUSLEGiliXEy/Q7ycRCEARBELpCgiVIfNVhIb1CEARBEPpCgiVIeKVbg8E7S4hcQgRBEAShLyRYgsTjEvIs449JsBAEQRCEvpBgCRJPWrNY6ZYsLARBEAQRDkiwBIkUwyIso0q3BEEQBBEeSLAECTeiyGNYXP/JwkIQBEEQ+kKCJUicKpVupaBbMrEQBEEQhK6QYAkSVQsLuYQIgiAIIiyQYAkS9Tos8tcIgiAIgtAHEixBop7WTC4hgiAIgggHJFiCxGNEUSscF/n9IQiCIIh4hgRLkDBQ80OCIAiCiBQkWIJECroVllEMC0EQBEGEBxIsQeIzS8jZFHtEEARBEPELCZYQUQ26JQsLQRAEQegKCZYg8ZTm96506yDBQhAEQRC6QoIlSDwuIc8y3kuI9ApBEARB6AsJliBh8FYl1K2ZIAiCIMIDCZYgUbOwSC4hKsRCEARBELpCgiVIuCQRY1jIJUQQBEEQ4YEES5CoWVjIJUQQBEEQ4YEES9BoNz8klxBBEARB6AsJliDxVLqlXkIEQRAEEW5IsASJFMMipjUbeAwLKRaCIAiC0BMSLEHCRYlRFsPi+k8xLARBEAShLyRYgsTj9vF2CTlIrxAEQRCErpBgCRJuYVGvdEuKhSAIgiD0hARLkHjqsHgglxBBEARBhAcSLMEi1WHxdgnV251NsUcEQRAEEbeQYAkSNQsLdwltPlKK4vK6iO8TQRAEQcQrJFiCRC2GRcwYWrD+cIT3iCAIgiDiFxIsQaLWS0h0DzVQqhBBEARB6AYJliBR6yVkEp5Q4C1BEARB6AcJliBh8C1IqJ8QQRAEQegHCZYg4QYUoxC4Umd3SI8dZGEhCIIgCN0gwRIknuaHHmrqPYLF1kCpzQRBEAShFyRYgoS7hMQYllpBsHy87Tg2HDwT6d0iCIIgiLiEBEuQeCwsHsVS2+CQrXPDm5siuUsEQRAEEbeQYAkSKa1ZsLD0bZ/htR4F3xIEQRBE6JBgCRKpcJywbETXLLx982BM7J0tLWtwUCwLQRAEQYQKCZYgkewmBvnyi3tm48XrB0jP7WRhIQiCIIiQIcESJGoxLByTkOpsJwsLQRAEQYQMCZYQMXjrFZiNVKKfIAiCIPSEBEsQMKEonIpegcFgkEQLBd0SBEEQROiQYAkCsYitUc3EAsBsci2noFuCIAiCCB0SLEEgNjbU0CtIMLq+Wgq6JQiCIIjQIcESBKIEUQu6BTwWFgq6JQiCIIjQIcESBEyuWFQxm1xfLQXdEgRBEETokGAJAobGXUI86NbuJAsLQRAEQYQKCZYgEC0sGnpFCLolCwtBEARBhAoJlhAxaJhYpKBbimEhCIIgiJAhwRIEgVhYKEuIIAiCIEInKMHy6quvolOnTkhMTMSgQYOwdu1azXWLiopwww03oEePHjAajZg9e7bXOgsXLoTBYPD6q6urC2b3wo5/MSw86JYsLARBEAQRKgELlqVLl2L27Nl46KGHsH37dowaNQpTpkxBYWGh6vo2mw2tW7fGQw89hH79+mluNz09HUVFRbK/xMTEQHcvIsgtLBouIRNVuiUIgiAIvQhYsDz//PO47bbbMGvWLPTq1Qvz5s1DXl4eXnvtNdX1O3bsiPnz52PmzJnIyMjQ3K7BYEBOTo7sL1qRZTVTWjNBEARBhJ2ABEt9fT22bt2KiRMnypZPnDgRGzZsCGlHqqqqkJ+fj9zcXFx22WXYvn27z/VtNhsqKipkf5HCn0q3JkprJgiCIAjdCEiwnDlzBg6HA9nZ2bLl2dnZKC4uDnonevbsiYULF+Lzzz/H4sWLkZiYiJEjR2L//v2a73nqqaeQkZEh/eXl5QX9+YESiEvIThYWgiAIggiZoIJulam8jDHN9F5/GDZsGH7729+iX79+GDVqFD744AN0794dL730kuZ75s6di/Lycunv2LFjQX9+wIiCpZGg25LK6AwcJgiCIIhYIiDBkpWVBZPJ5GVNKSkp8bK6hLRTRiOGDBni08JitVqRnp4u+4sUsiwhjXW2F5YCAP751Z4I7BFBEARBxDcBCRaLxYJBgwZh5cqVsuUrV67EiBEjdNspxhgKCgrQtm1b3bapJzKXkIaJpaLOHqG9IQiCIIj4xxzoG+bMmYMZM2Zg8ODBGD58ON544w0UFhbirrvuAuBy1Zw4cQLvvvuu9J6CggIArsDa06dPo6CgABaLBb179wYAPPHEExg2bBi6deuGiooKvPjiiygoKMArr7yiwxD1x4/ehxjVLQtr95+JxO4QBEEQRNwTsGCZPn06zp49iyeffBJFRUXo06cPvvrqK+Tn5wNwFYpT1mQZMGCA9Hjr1q1YtGgR8vPzceTIEQBAWVkZ7rjjDhQXFyMjIwMDBgzAmjVrMHTo0BCGFj6YH1lCcyZ0x9r9Z5DbMilCe0UQBEEQ8YuBiXffGKaiogIZGRkoLy8PezzLmSobBv99FQDgyNOXqq6z41gZrnxlPdq3SML6v1wc1v0hCIIgiFjF3/s39RIKAn8kHre8xIkeJAiCIIgmhQRLEPAsIV+Z3Eb3iyRXCIIgCCJ0SLAEg1uFGP2oPeMkCwtBEARBhAwJliDg/Qx9yRXJwkJ6hSAIgiBChgRLEPjjEuKvUbNmgiAIgggdEixBwCQLi7Zi8biLSLEQBEEQRKiQYAkCSYKQhYUgCIIgIgIJliDgqcq+Y1jk6xIEQRAEETwkWIJAcgn5TBJyvUgWFoIgCIIIHRIsIeA7hsX1nywsBEEQBBE6JFiCwB8Li4HSmgmCIAhCN0iwBIGU1uxjHcnCEv7dIQiCIIi4hwRLEDA/Kt0apBgW/SULuZkIgiCI5gYJliBwssZL3XqaH+r72eW1DRjz79V4/PNf9N0wQRAEQUQxJFiCgGsQXy4hTx0WfRXLd3tOofBcDRZuOEKWFoIgCKLZQIIlCDxBt41XutVbUmSlWqXHReV1Om+dIAiCIKITEixB4X8vIb2tICbhQ5fvKtZ12wRBEAQRrZBgCQI/QljC1q1ZLET3/d4SfTdOEARBEFEKCZYgkGJYfGYJudA7hsUhbK/aZtd12wRBEAQRrZBgCQJ/LCwIUx0WUQDV1Dt03jpBEARBRCckWIJg4YYjAICz1fWa64TNJeQkwUIQBEE0P0iwBMHinwobXUe0vugZeCvGsNTUk0uIIAiCaB6QYAkTYhVcPTs2O8jCQhAEQTRDSLCECTEeV08LC1PEsDj1VEMEQRAEEaWQYAkThnBZWBTip7aBrCwEQRBE/EOCJUzILCw65gopxQ+5hQiCIIjmAAmWMCHGsOiZKaR0AVVRLRaCIAiiGUCCJUzIs4T0266yEN3pSpt+GycIgiCIKIUES4D4G0ArzxIKn0uoqLxWt20TBEEQRLRCgiVA6h1Ov9aTx7Doh9IlVEwdmwmCIIhmAAmWAGCMoby2wa91RcGir4VFvq0iEiwEQRBEM8Dc1DsQS9yzaBu+2lns17oGhCfoVpnWTBYWgiAIojlAFpYA8FesAIAxTIXjuEfI7P6A5b8U44PNx3TbPkEQBEFEIyRY/KTBz9gVjiHMac3tWiRJyx74+GeUVJKlhSAIgohfSLD4yTkfnZnVMIY5hqW9IFgAYMexct0+gyAIgiCiDRIsfnKmKrB6JzILi477wZsftk6zKpYHZgEiCIIgiFiCBIufnKkKzMICeDKF9LSwMEUMCydAjxVBEARBxBQkWPyk8Gx1wO+RJEUYsoRECw4A2MnCQhAEQcQxJFj8ZH9JVcDv4dVu9ezWzK01JiPQITPZazlBEARBxCMkWPzk8JkgLCxuI4iu3Zrd6sdoMGDJHcOk5eQSIgiCIOIZEix+EmiWEOBx2+hrYXH9NxoNaNciCRf3bAOAgm4JgiCI+IYEi5+U1fhXkl+ER5noWTjOIVlYXM9N7gdkYSEIgiDiGRIsflLhZw8hER7Domd4CRc/Jve2+X+ysBAEQRDxDAkWP7A7nKi02QN+nxTDEsYsIY+FhYJuCYIgiPiFBIsfVNQFLlYAMUtI/15CRqVgIb1CEARBxDEkWPygPAh3ECDEsOi3K7K0Ztd/cgkRBEEQ8Q8JFj+w2R0AgFSrObA3hqHSrZjWDFDQLUEQBNE8CPAO3Dzh8SHJFhMGdGiBspoGjOyahbE9Wvt8XziCbsW0ZsATdEuF4wiCIIh4hgSLH3Bvi8lowLu3DgXgXRpfDU/QbfjSmrlwsVMQC0EQBBHHkGDxA56ZYzQY/BIqHMnCouO+KNOazVLQLQkWgiAIIn6hGBY/4FYNk9F/sQJ4gm71dNdopTU7Ka2ZIAiCiGNIsPiBk8ndMP5iCGMMCxcq3IpjJ8FCEARBxDEkWPxAihsJ1MIS1iwh13OziYJuCYIgiPiHBIsfOBVxI/5ilIJu9d8Xo8LCQpVuCYIgiHiGBIsfiFlCgWCA/i4hXm/FqAy6JcFCEARBxDEkWPxAzBIKBMnComOekDJLyEiChSAIgmgGkGDxA2ewWUJSUTf99sWTJeR6LnVrphgWgiAIIo4hweIHoQbd6lU4zu5w4rOCkwAAm93lG+JBtw4qHEcQBEHEMSRY/MAhuWECe58nS0if/ThTVS89zkyxABCCbsnCQhAEQcQxJFj8IFiXkCfmRR8x0SB0OLy0b1sAFHRLEARBNA9IsPhBsEG3nkq3+uwHLw6XZjUjPTHBtU8kWAiCIIhmAAkWPwi2NL/e3Zod7vxqk+Cb4g/JJUQQBEHEMyRY/EAqHBdwbX75+0OlwR1YazZ6fjaTyfWYgm4JgiCIeIYEix8oi7X5i94WFrtblCTILCwUdEsQBEHEPyRY/CDoOizu/7qlNXOXkLAfbgMLdWsmCIIg4hoSLH7gCLJbs2Rh0Wk/eNBtgklwCbndQ9StmSAIgohnSLD4gTPYLCHdY1jcxeLULCzkEiIIgiDiGBIsfhBqaX79soS898NiMgEAbA1Or/Uf/WwXxj//A6ptdn12gCAIgiCaiKAEy6uvvopOnTohMTERgwYNwtq1azXXLSoqwg033IAePXrAaDRi9uzZqut9/PHH6N27N6xWK3r37o1ly5YFs2thIejS/O7/elk/PEG3np8t2eISLDUNclHidDK8u/EoDpRU4fu9Jbp8PkEQBEE0FQELlqVLl2L27Nl46KGHsH37dowaNQpTpkxBYWGh6vo2mw2tW7fGQw89hH79+qmus3HjRkyfPh0zZszAjh07MGPGDEybNg2bNm0KdPfCAs8YNgWaJeT+dnXLEnILJ7OQJSQJlnqHbN3iijrpcaLZpM8OEARBEEQTEbBgef7553Hbbbdh1qxZ6NWrF+bNm4e8vDy89tprqut37NgR8+fPx8yZM5GRkaG6zrx58zBhwgTMnTsXPXv2xNy5czFu3DjMmzdPcz9sNhsqKipkf+EiWJcQt4SIJfVDwa4Sw5JsMQMAamxywXL4TLX0uM4uf40gCIIgYo2ABEt9fT22bt2KiRMnypZPnDgRGzZsCHonNm7c6LXNSZMm+dzmU089hYyMDOkvLy8v6M9vjGBL8ycluCwbtQ36CIYGp3fhuGQrt7DIXUIVtQ3SY4phIQiCIGKdgATLmTNn4HA4kJ2dLVuenZ2N4uLioHeiuLg44G3OnTsX5eXl0t+xY8eC/vzG8AS7Bva+RLdgqdNJsPDS/P64hMQs58o6EiwEQRBEbGMO5k0GhaWBMea1LNzbtFqtsFqtIX2mvwTrEpIsLPU6WVik0vzeLiG7k6He7oTF7C7VLwTOVNvIJUQQBEHENgHZDLKysmAymbwsHyUlJV4WkkDIycnRfZt6EqxLKFFyCekVw8KDbr2zhAC5W0isrltl87iHCIIgCCIWCUiwWCwWDBo0CCtXrpQtX7lyJUaMGBH0TgwfPtxrm998801I29SToC0sFtfXq7dLSOwllGAywuIWMKJbyOEUBQtZWAiCIIjYJmCX0Jw5czBjxgwMHjwYw4cPxxtvvIHCwkLcddddAFyxJSdOnMC7774rvaegoAAAUFVVhdOnT6OgoAAWiwW9e/cGANx3330YPXo0nnnmGVx55ZX47LPPsGrVKqxbt06HIYZOqEG3egkW7hIyGeU6MzXRjHPV9Sgqr0W7FkkA5ILllJDiTBAEQRCxSMCCZfr06Th79iyefPJJFBUVoU+fPvjqq6+Qn58PwFUoTlmTZcCAAdLjrVu3YtGiRcjPz8eRI0cAACNGjMCSJUvw8MMP45FHHkGXLl2wdOlSXHDBBSEMTT94VnLQMSw6CRbe/DBBsR8Xds3C5ztO4ptfTmFQfiYAee2Xfacqdfl8giAIgmgqggq6vfvuu3H33XervrZw4UKvZf50K77mmmtwzTXXBLM7YYcF2fzQqnPQrVrhOADonp0KACgXUpnFoNvjpbWorXcgyUIF5AiCIIjYhHoJ+UGwpfkll5Bd36BbpUuIZ1OJLQAciu7Nell5CIIgCKIpIMGiQK0qLbdWBFqan1s0dLOwOLyDbgGPq0rUKEqrFncnEQRBEEQsQoJF4N2NR3Deoyvww77T0rJ6uxMrdrlSrgONYUlM0DdLyK6RrcSfOp3aFhblc4IgCIKIJUiwCDz62S+odzhx09s/Scs+LTiBk+WuLJtAs4Ss7qaD9Tq5hBxO78Jx4n7JXEIKfWJXLiAIgiCIGIIEi0Ca1TsG+Ye9HmuLUig0htVdddamU/NBKZbGoC5YHD5cQmRhIQiCIGIZEiwCdpWbeodWydLj8b0Dq7zLLSw2vSwsTD34V3IJ+Qi6VRsbQRAEQcQKJFjc1NudskyaT7efAADY3GX1fze2C3q1TQ9om7yvj14uIa5HlMG/PKbly5+LcO1/NuBslU2W1gyQhYUgCIKIbUiwuLnhzR9lz2cvLQDgcedw904geFxC+sawKC0sYpPIzUdK8Y+vdkNZ+oayhAiCIIhYhgQLXKnMW46Wqr5W57aw8EaGgWBN0DmGRSO9WhnT8sm2E5QlRBAEQcQVJFgAnK60ab4WmoXFJXLOVNXrktrsacIoX658DgA/HT4ne04xLAQRH6z89RRW/FLc+IoEEWeQYAFQ5E5bVsta5u4cLj4CQRQ5f/1kZ3A7J+CPS4iz7sAZ1fcSBBG7lNc24PZ3t+DO97bqVpCSIGIFEiwAfjlZDgAY2KGl12vcMsKLwAWCRRAsn7iDeENByyXkTwVeqsNCELHPsXM10mNqt0E0N5q9YDl0ugr//Go3AOCiHq29XtfLwqIHTq1Kt358DFlYiHjA4WQoqaxr6t1oMo6X1kqP9aqgTUSWxz//BVe/toEsZEHQ7AXLG2sOoa7BiaGdMnHH6C5er3sES/AxLHrBjSRaheN8QVlCRKxTbbNj6ivrMfQf32LLkXONvyEOOV7qsbDolX1IRI6TZbVYuOEIth4txeZmegyHQrMXLAXHygAAt4/qDIvZ6GVlsblnMdYgXELKJoWhomlh8UOwkIWFiHWeWb4HO0+43Ldf7WyeQadnquqlx3plHxKRY6uQjXqqovlaCoOlWQuWersTB0qqAAC927mKwj1yWW8AQHqiq0w/n8UEk9asFgwbClpBt0rBMqBDC6/3UpYQEcus2Xca7248Kj3ffky9DEG8U1YjCJYGsrDEGuW1DdJj0b1H+EezFixltfWwOxkMBqBdRiIAIMGdI9zg9r9wC4tFLXc4AALt9KyGZtCtYteSLd7iiiwsRCzDzee831eFcOFvTpQKgoViWGKPyjq79Li4nCwsgdKsBYtY6p5bQ3hmT4PDNXupsrkOsBSVxoj+8Ny1/QAA2WnWUHYVgMclpNQ+SktOkoo1iCwsRCzDXSHDu7QC4DkvmxulNR6hRjEssUdFnef34/cYwn+atWBR637MLSx2J4PTyaQLY1picIKlb24GAKBGh9mQVvNDpcUlyeK9rw4KuiVimDNVruKOHbNSAABVdc1TsMhcQiRYYgqb3YHXVh+UntMkMnCatWBxSgLAs8wsBMpW1DWAH1PBCpYkt3umRocUNi6wvErzK37FJJUAYarDQsQyZ92CJd/dPb263tEs3ZwVtR6hRi6h2GLNPirmGSrNW7C4JyiihUWMVeFmaKNB3c3iDylua0e93RnyAcoFljIexh+XEJ0cRCxzttp1LnZslSItq65vflYWcVZOFpbYol7xe1GpicBp3oJFJYg1QRAs45//AQCQajUHnfGTJATA1oR4gdXKElJaXBJVgm7J/EjEMtxCmZlikSYVzdEtxJgoWMjCEkucq5b3rKNJZOA0a8HCY0LE+73JaPAKak1LTAj6M6xmo7S9UN1CXJA31q2ZLCxEvMEDFBNMRqS63bOVzVCwOAXB8tCyXbhk/lrsO1XZhHtE+IuyyS5NIgOnWQsWpuFiSVDkCQfTR4hjMBiQ7HYLhSpYpLRmxe54x7CoW1i2HDmHa/+zAbvcxbcIIlZocJvTLSYjUqyu47s5uoSU97hfiyowf9X+ptkZIiBOu+OwOrkDx2kSGTjNWrA4VGJYAO+aKyzE48oTeKuTS6gRC4taHZa/ffErrvnPRmw+Uoq/fPJzSPtBEJGG10VKMBuk81MZE9Ac4BaWbm1ScWX/dgCAL3cW4YMtx5pytwg/qHBbBFulWABQIkQwNGvB4tRIE04wKy0sofUESnELiFCbXWkF3SqfN7a/dKIQsQRjDPWCS8ji7tHVHAUL3KfumzMH44HJPaXFD3xEk5Bop8ZdIiM9yRViQEG3gdOsBYtDoxBbokKwBJvSzEnSyyWkWZpfvl5jgqVnTlpI+0EQkUT09SeYjLC4Sw80R8EiTbIMBqk6NxEbVNtc1/8MSbDQxDFQmrVgESvdiiizbHJbJof0Ock6uYScGvsrZjAZDfLO0r3aunokiVaY7HS60BGxg1gR1GIyelWjbk7wa4DBoH+vMiK88Jgr3qeOYlgCp1kLFk+WkO8YkAcm9wjpc5J1Kh6n1a1ZFDBmo1Hm0rppeD4KHp2Ag/+8BHeO7gxAsioTREzQYBctLAZJsNQ3S8Eiz2x85YaBTbg3RCBUu11CkoWFXPMB06wFi1ZMiJhlc2nftiFbJPj29MoS8hV0azIaZEHDRqMBLZJdQV5cmDlJ2RMxBBcmBoP8+G6OhdO4VZif8/3yXK0/QslkJCJDtfv6z2NYyMISOM36KNdqJijGgFjNoX9FvHFiyEG3GvurbC0gpmWbFO4iwDs1kiCimcU/FQIAEoxGGAye47s5xrAwyCct/D+d09FPjdLCEudBt4wxLFx/GPe8vy3kex+neQsWPlvxYWGxmkPLEAL06yfk0LAIGWUuIQMShH5IopgxSIKFrm5EbFBwrAzPr9wHANJxTTEsnskHP/cZndNRjdPJmp2FZcnmY3j8f7/iy51F+OnIOV222awFi1ZdEzGGRQ8LSzJ3CTWEpzS/3CVklFlYjDILC13ciNiiuLxOesxjs6QYlmZoYXEq4u7Iahob1AqNKtMTm0eW0Le7T0mPefxOqDRrwcJUegkB8v4/Vh18w1LQrU2noFvF/op17hJMBuQI6Y7tWiRJj/lFLr5PEyKeSBdKCnDBbm2mgoUxJsWw8EuAFJdGk5CoRoy34pWa493CUiWIFL1cQqEVGIlx1HoJAcoYFj1cQnqX5tdOa26dZkVWqhWr5ozGmap6DM5vKb1mJJcQEWMkypqHus4fKYalmbmExNPWqLCwMOYSNJTqHJ3wRpUJQoxhQ5xnCYn3u5oGEiwhI9U1UQgA0XylR/Q9V9S1IbuEXP+VLizR4pLjzmjq2iYNXdvI328ABegRsYV4k+YzUktzFSzCY2UMC+D6rkivRCf1Qi8ss/vHc8R50K1oYamjoNvQcWrEsPR2F1sDgIm9c0L+HL3SmrXSsMX9b+uj+qVnNkaKhYgN1KyBzTWGRfwuDIosIeXrRHTBj1Vrgkm6fsd7DIs48Q/13sdp5hYW9SDWawfnITHBhEnn5aClu1FVKOjWrVkqHCdfLmYCtfFRM4aPk65rRKygVjOIBItn8mEwiq9HeIcIv7HJLCyuHy3eY1iqhZjNWnIJhY5WL6EUqxnXDe2g2+foVprfj27NLZO1BRalNROxhto1vbnWYRFPW7KwxBaSYDEbYTLFv4WFMSa1IgCA2hDvfZzm7RLS6M2jN5Gsw5KepK1BqcgUEWuIN2GeMWRtpqX51Sws4qWA9Er0Ui8IlgQphiV+f7CaeofseNTLJdTMBYu6xUJveLfnitrgVSZjTCqUlaDwCYkuIZ7jrwYfJc3EiFhBPFYX3T4MgOf4b269WNSzhMjCEgtwcW0xGaUJp8PJ4jaesLSmXvacXEI64IlhCe/ntE6zAgDOVttgdzhhVgah+IHN7pTS4FIT5T+beNFKS2zcwhKn5wgRh/BZaO+26ejT3tU3x+w2qTe3SrfyoFv5f+XrRHRhc9+wrQmeGBbAdXybTeGdMEcSu8OJwnM12HKkVLac6rDogFalW71plWKF0eByxZytrg+qmWJlncc6k2KR/2wmmWDxYWGhLCEixuCHqugGTXBf8OM5BkANZ6MWlkjvEeEvMguLIFDsTgYdSn1FDX/6cAc+LTgp9Utqk2ZFSaUNDTodnM3aJaR2MQwHJqMBWakuK0tJhS3g95+tsmHIP1YBAFKtZq/9FQ+FVCvFsBDxg8dt61nWXC0s4kSDfx3yOix0YkcrshgWQbDYGuLrGP604CQAoLy2AQDQPTsNgO+aMwXHyjDjrU1+bb9ZCxZuYYlEdUheLn/L0cCaQDHGMOjvq6TnVSo9GcR+Ry2SG7ewkOmYiBXU+meZm2kMi7qFRf11QpuVv57C4TPVEf1MniVkNRthNZukgqQVdQ0R3Y9I0ykrBYDvAOOjZ6ux/ViZX9tr1oJFKsQWARfixN7ZAIBvd5cE9L4TZbWNrpNiNWPx7cPw0V3DZW0FlFAMCxFreLoTe05SC1lYvHoJATQR8Ye1+0/j9ne34KJnV0f0c6XCcW7/D3eZcEtEvNKldeOCJRCvAwkWhD+GBXCVygeA46U1+GDzMb9rsvjbb2J4l1YY3DHT5zrUS4iINVRdQu4YFr384rECH67BIBcqdF77z4aDZ5vkc0WXENA8BEtGUoLUfNdXvFlJZZ3ma0qauWBx/VdWug0H3G1z5GwNHvj4Z/R+dAWOl9Y0+j49+00YyMJCxBhqkwoew2JvphYWrcKR8Xpel9c0oFIn14lYbFCtinK44M0PeR+seBUsyUKz0h45aVIJAl8WllNkYfEPrUq34cBi9v6qX//hUKPvEy0seZlJeOqq84PeB0/QbZxe2Yi4Q80l1FzrsEgWFsXyeD6vy2sb0O/Jb3DZS+t02R4XDgBQFkGxUFrj+qwMd4whFyzF5f5bF2IBXsIDANq3SPL0TfJxrvoT9sBp1oKFz1jCnSUEqAsWfzpB8x+6bUYi1j5wMa4PoWWAJ+g26E0QREThs2DxHOXdbptLDAsfp5YLO57P61W/ngIAHD1bo4tFpLTaI1LOVQeesRkspytdn9XGfUPPSHK1UHl+5b6I7UMkEM/TtESzrEieGvV2J3adKPd7+81asEQyS8iiUiyuhY++P5wGt0tIj+JC1K2ZiDX4TVo8RRPMPIYl/gXL37/4Ff2f+AZHz1ZL5QuUlyvJwhJHisXpZFjxSzHeWndYWubQ4bolZuWcrar3saa+8DgNboEY1tkVbxg/JeNciJOI64Z0EDpTq5+rB09XwWZ3Ii3Rv2I0zVqwRKqXEKBuYfFVM4XDLSwJOpTjNcSx6ZiIT1RdQsbm4xJ6a91hVNc78I8vd/tofur6Hy+ntd3hxO3vbsGd723F7qIKabkevXfEnjYVdfo05POHEreFhQuWMd1bAwCq6+1xNYHk5+RL1w9A73bpkjVU66crrXaJxjZp/hVTbdaVbtUyEMKFmmDxx6TNAwv1cFtJwXkhb4kgIoOqS0hKa24+R3LhuRpJkCgvBfEWw7L5SCm+3eNd/iHY8dXbnfjHl79idPfWqBbqWOkVyNsYNrsDJ0pdcRrtMlxZM8nuyaqTuWq0+CpHEUvwe1q37FQAaNTCwoOOffXAE2nmFhbvolThQs0l5E+3WZ4OFkz/ISWe5ochb4poImrrHZj6ynrMXrK9qXclIqhNKnilUK2LYDxy6Ey14B5Tj2HRw2USDaze6xIrg/Jb4pUbBkrLg7WwfFpwAu9sPIrb3tmC6npRsETGwlJQWAab3YmsVCvyWyUDAJIEgVKtUgw0VuFZWDwwnpcgcGhMLnjgc0ayf7aTZi1YuF6IRB0Wq5qFxd74Ccgvygl6xLC4dyGeTJDNjUU/FaLgWBk+LTiJOp06oEYzDrW0Zl6Hxd58BEu93Ym31rmyCr1iWIw8rTk+zuvv3YLlphEdMcFdcBMAgtWnZULn4GPnPBkpkbKw/HjIVd18WOdMSWyajAZJtNTo1BgwGmhQhDB4LCwagqWGW1gaj+cEmrlg8VS6bZoYlnpH4wcqPwDMOrqE4sV03NxwOBme+2av9PzYucbr+DQFPx8vw7bC0sZX9APVGBYp6Db+j2PegwwA/u/HQgDadVji4euostmx71QVAODCrlkyV2CwFiSt5IZIWVh+POQqVjescyvZ8hSrS7BU+1lENBaQJthm1+/G3bda1rGt7lY16UnkEmoUKYgtAt+CqmDxY4bIg5h0cQlJ2QQhb4poAvYUV8hmY0fPRp9gqWtw4IqX1+OqVzfoYuqWiqUJh3+CVNsh/g/kerv3pCaeY1gOlrjESlaqFZkpFtlYgw661XhbZQRcMXUNDkm8ewsWlxskXlxCjDGPhcV9v+LHppqFpd7uxCp3q5oMimFpHLXZW7hQjWHxR7Do6BLiW2BRGHbb4HDirve2YvK8NaoNHgnPDJtzpipydST8RSyEpUcVT4dKZgwX704WX6m8aqjFuWllCcXDRIRnBXVt4+pBYzAYAm49UGWzY9eJcunYqNVwnS7aVBh2N9qBElfabsvkBKmvDifZwgVLfLiExCB4TwyLtoVFvM5f3q+tX5/RrAWLmn88XJhNRq+ZUb0fWQ78IDDpYAaKZtPxugNnsPyXYuwprsTmI4F1tG4urN1/WvY8GmNYTpZ7YgT0mDmqTSrEmkTRUoulvKYBH289jpMBVO30Bz6puWVkR2mZZh2WOLCwfLWrGAAwokuWtKyx4mMijDFc8fI6XPbSOizdcgyA5zzp6A54FTlequ/vpYR/dkZSglewdIq7jH28WFjEIHg+wfaVJcSFpMVsRG6m92+jRrMWLJGsdAt4u3X8cwm5LSy6xLC4/kdjcF6FMBvfV1zZhHsSvfAW9ee1SwcA1EVZ0Gm93Yk/LC6QnuthcldLaxZrEkVLavM9i7bhjx/uwIMf/6zbNu0OpyTYWqV44jCUN754qsNywt1fbYjQyJULMn8ES3W9A4dOVwMAvt3tqpLLb4wjumbh39f0xRszBknr21RcbnpiVzl+OVZ3pXPRisYYi8rrsz+ISSSShcWkHYbAxVyiSriEFs1asHhSsCIjWJQCJbC0Zh1cQhG2sJypsuGV7w9IZal9YWvwfBdkYVHH5j7Bec2CaLOwvLBqn8xNpUdQo2qlW+FciJY4lnUHzgAA1u4/o9sNR7w+tBKCb5X3vngqCMlFicXsGaTJ6Ht8jDHpOBAzgrhwqXNfW5ISTLh2cB4mnpcjBTOHW/DaFTEdIiZFAcQzVTbc+NYmXPnK+qg5rgNBPF65K8inhcUdj5dk8b8GTbMWLDw6m/sSww0vx8zxJy2TH7h6BN1G2sLy1Fd78O8Ve3HDmz82uq5NONi/21OCmjiKnNcLfkHgjdPqGqLrovbFzydlz6t0ESyu/6JLyGQ0SDFh1VGSEtq+RZL0uLhCn4Z24gQnU7CweMWwuC8NcSFYVNz0pkYsLPctKcDIZ75DZV2DLG6KuyelmbzQu02q5RNuweLULvyZILi6DpRUYvDfV2HDwbP4+Xg59p6KDStzeW0DzronKbxoXILJIIloXoJALd6M/y5JARTNa9aCpcYd7JQcgMILhXduHYrtj0zA36b2ARCYhUUPl1CkLSw8ZW1/SRXeWuu7M7VNsBY4WWT7fMQCjDHJJeQRLNFxswZcHVfFGhcAUGULPehWrfSAwWCQ0iDFGXW4eOnb/bhk/lpsPaqdqi1OAlbvPa25XiBwwWIwAC2EtM94Tmt2SGUcPLcmYyMWls93nMSpChse++wXWXPDugYn7A6n6o2RWzz8uQaHgq8sT7FGCU/l5pT4YZVuahhjmPrKeox85jscKKlStSb5SkvnE65Aqvw2b8Hinp0l+9HTRw+sZhNapliQ5v48f2JYGnRMa460haVP+wzp8d+/3O1zXZviu9AjwySeaHAwKUYhPcl1/ITb/x4IxeXewYu6uIQ0Sg+0SHbdwCNxnDy3ch9+LarAE//7RXMdMZ6ooLBMl8/lN1OLySizsCiRWm74eV7bHU7sLqqIquOHoxbz4Qm69V5fHPMn20/gxW/3y16vaXAIFhbPjdEsWVjCLFi4S19lwumpUeL0skY+u2Kv1/rRRqXNjsNnqlHX4MT453/Aj4dd9WY0BYtCUdeq/C6N0ax7CXGXUEqELCwc/gOV1TY+O5RcQroWjgt5U36hVntGC6V4I8EiR5wJRqNLqEolNfNcdejWD63SA/w7qAjzcSJasXwF59cKrik90vIZY3hrratTscVsRAchw0WZzs6/Gn/O6/2nKjHhhTUAXC7qJXcMD3lf9cShErOnFXRbbbNj8vw1smU/KeLfqm126cZoFW6M3KWoVYFVLxw+gm55DEuDg6HBIT9mfjlZgXq7M6BraKQpr5Gfez8fLwMgFyzifUv5XdeSSygwJAtLhAXL+bkuy8PuospGb8wNugbduv5HytettJr4mgEq1y2rIcEiIrrM0qIw6JanZg7p2BIPX9oLAPDq6oMhz2C1Sg9kSC6h8Bwnpytt+POHO/C/HZ64HK3u6owxWa0PPbKjVu87jYUbjri377LOcpTnSiBpzVysAK6S8dE2MVC3sLj+K8e3avcpLzekkmqbXapzkmr1trCE3SXko46WWKNETeT+349HvZbtKa7QZSKgB8pzr9T9XByrzMLiULqEvGOLGoMECyIXdMtp3yIJ3dqkwuFkWLj+iM91HU5uYdGv0m2kYvNsihuqrwBJpXnaH+tTc6JeCGjjUfXRJFj4BTfVakbHVp4CWSt/PRXSdqVKt4rrPRcs4brhPvXVbny49Tj+/JEnTVkrQPO/6w7Lnlfp0KNm6xFPvAz/brWsrP4WVlMLWt15vDzIPQwPkkXCj6BbZXq3GlU2h3QtaZHkcauZFRk64cLuo46WWYhh4b/xrAs7ISc9EQAwb9U+2fq/nqzAJfPX4vo3fgy+6q+OKK/RpW4hJXMJCb+RdwxLhLKEXn31VXTq1AmJiYkYNGgQ1q5d63P9H374AYMGDUJiYiI6d+6M//znP7LXFy5cCIPB4PVXV6dPtL0WNVKWUORbe980oiMAYJPb76eFJ5BJvzosTWVh8WW+V7qEVoV4o4s3eNq31WySGmlGlUvI7YNPsZpxXvt0afmpEDNmpEq3ips1z8pZs1+fAFclv5ys8Fqm1R1aGZ+lh0tIrTqrWmosIMaw+N6mmrgrjUDQciCouVD4b6+84fmTiFBjs0uWgIxkT+Aydwk1hNnC4vA3hoUL/kQzrh2cC8Dbxff5jpNwMmDvqUpsiYLSD0oLyzlJsMh/Oz505fnD3aiJ5jAKlqVLl2L27Nl46KGHsH37dowaNQpTpkxBYWGh6vqHDx/GJZdcglGjRmH79u3461//ij/84Q/4+OOPZeulp6ejqKhI9peYmBjo7gVEta1pLCwA0NftFtpTXOnTVcIvMml+9lrwhb8XNr2wKW6ovoIwlUXRtvjIyGiOcAuL1WyUYqDqoihoslqwsLTNSEJepktQhBosrhXDct3QPJiNBqw/cBbbdWq0yKlrcKimlfpbs0OPdG7RenZZX1fZcq1Ji791WNRcCdHmElKLYZHqsCju4AXumAk18t0xPz/sOy1VsxUzrfj2wy1YGpzaMYhilhA/ZlKtZqlDdXqi/L50oMRzTJ6KgiwiZYZemeQSkp/z3JqltArVRKIOy/PPP4/bbrsNs2bNQq9evTBv3jzk5eXhtddeU13/P//5Dzp06IB58+ahV69emDVrFm699VY8++yzsvUMBgNycnJkf76w2WyoqKiQ/QUCY0xKu9TyTYeTbm3SALguIr788LzoWus0q+Y6/hL5GBb5DbXCh6mcC5aRXV0luWujpL5GtMDFn0UULFFkYflur6uJGW/o1i+3BYDQbwha1ahzWyZLN/LfvLoBP+zTz9Ly+Y6Tqsv9HcvJ8jqUhGhZ8qR8GvGva/oC0A5i91hOfW9TLQX84U934QN3CfumYOvRczh42pPSq1a3RM0lVFvvwOs/aJdKyGvpEiyvr/GskyETLJFxCakJMGkfBLeUKPg9ExL58XZWEJyRSOdvDC8LS423SwgQhJniu+ZWpUAm4wEJlvr6emzduhUTJ06ULZ84cSI2bNig+p6NGzd6rT9p0iRs2bIFDQ2eAVdVVSE/Px+5ubm47LLLsH37dp/78tRTTyEjI0P6y8vLC2QoOFtdj7oGJwwGIDsjdDEQKEkWk1Rt8YSP/iM8I0BsMx8ske45onQJ+QoW411pebqq3cn8SvuONOGekWnx7DeuNMei8jqplLUyRqipKKmsw3Z3Ki+/MPOba6i/Ib/gq4UrXDUwV3r8wEc7dIvpWbv/jOryQG5uk+ataXwlH3Dr2YOTe0oWYK04Nn/P6z9+uEN1uZ7tBALhRFktrn5tI8Y99wMAlwWFaxIx9kHNJeTrmgkAORne1nlRsFgiZGGxq9SV4YgWFl5ssFWqVXD5OrDrRDn+sHg7Vv56SooRAaIjKaHMbZ1r6/6utSrHa1mzKuu498B/g0FAguXMmTNwOBzIzs6WLc/OzkZxcbHqe4qLi1XXt9vtOHPGdWHo2bMnFi5ciM8//xyLFy9GYmIiRo4cif3796ttEgAwd+5clJeXS3/HjgU2S3h3oysCO8VilkXgR5J2LVw/dFG59mxMVwuL+3+kwrW4YOE1JHwVQ+Lrtkz2BMZFk5Wlpt6OJ//3K/o98Q0On6mO6GczxmQWBI+FJTq+n29+8cQbjXQ3rdMrRsBXR/XR3Vvj5RsGAABOVdi8gl+1KDhWhn5PfIN73t/mJaicToZ17riYsT1ay14LpNFiaU1DSJ2kbSo1KhLM6i4hf+srHT3r6tPTq206bruwk7RcrNIbSY4K51FZTb1MkIg3eLPkEnI9/3pnEd5co25dGd+rDeZf1x+dslK8XjPL0m3dx2eYg1ftPlxCfNmx0hopZiq/VbL0m9vsTjzy2S58vuMk7lm0TVZBOSoES41csHCUFhZ+f1VahMVAfX8JysGsjM5mjPmM2FZbX1w+bNgw/Pa3v0W/fv0watQofPDBB+jevTteeuklzW1arVakp6fL/gKBFxiKQKNmTdpluC4UvOGXEsYYzrgrvmalaheO8hc+U4l0llCeuxOnLzM5d3mkWM3SiVzTEB3l+b/eWYTej67A2+sPo6begX+v2BPRz5+vKIalZTJuKorcReOuH9oBo7u7bvJ6WVjUKt2KXNa3Hab0cbmP/elZBQCLNxWivLYBX+4s8nKHFJ6rQWlNA6xmI0Z0aSV7jc+W//7Fr5j1zmZJkFzQydVyQ+yoDIQW0Cq6hDgJGhYWKYbFx1ddKbhjX7yuv8xFIWZ1RZIaYUJy9GyNzOVjUqvDwhh2nSjH797fJnViVvLWTUNwZf/2UgwLhwtbTqQLx6nVYeH78OXPRdKyDpnJUhCqw8mktO16u1N2w4+GLErulrq0bzvZcqXrkh/DyhCByrowC5asrCyYTCYva0pJSYmXFYWTk5Ojur7ZbEarVq1U32M0GjFkyBCfFpZgOHKmGjcv+EmWgXLLyE4+3hFeumWnAgA2CymMIjX1DinY0lelS39pqiyhDpJg0b6heIKLzVIQVk2UWFjuWbRN9tyXRSwcHDztmYkuuHmIdAGIFgsLv/CIHYU9pc9DO9acGmnNIryicrWf2TmiCfrn42Xy2Aj3d5qWaEaqVe5btzucaHA48da6w1i1uwS/FrlmxXZBuPx5Ug9p/VM+jvfG8HSy9S4nr8Sf85pbBbNSreiWnSb7rlKsTWNhFl3ER85Wy34H0SIhBt3+eMh3ViWHpwZz+rZvIXseqSwh36X5vZclJpikLs6Adiq7smhbJKmtd+DiZ1fj2z2uuLV2GYk4X6hqrtxnrZg7MTPKXwISLBaLBYMGDcLKlStly1euXIkRI0aovmf48OFe63/zzTcYPHgwEhLUg20YYygoKEDbtm0D2T1VNh06i9+8uh5TX1mPu/5vK1bvPY3fL94uHbDTBuc2soXwMbZHG9c+HlZPUeM+QovJGFA1QC20sgne33QUz32zV/eS/Vyw5LV0WZJOVdbhk23HseuEd+0HfvFqlWKR0szHPfcDPtp6XNd9Cgaly1CZ/RRuuJh7+qrzcVHPNjKXUDS0oueCRRQCfJYVuktIPa1ZJCVAgSsWC/tgy3E8tGyn5zW7pxx+kkV+eWxwMhSVecQqH6On/LoRd43pIk0uTlUGL2x5DIt/LqHGK1grRaWYsWd3MDS4xVgk2VPsyXr58dA52eerpjU7GXYX+dcUUDnBy0qTP/fEVYTbJeQjrVmx7J+/OR8ApBgWwNsyc6k70Fwt7T1S7C6uwCHBnZeRnCCLW/F2CbknWAoLS6HbRRm2GBYAmDNnDt566y28/fbb2L17N+6//34UFhbirrvuAuCKLZk5c6a0/l133YWjR49izpw52L17N95++23897//xZ/+9CdpnSeeeAIrVqzAoUOHUFBQgNtuuw0FBQXSNoNl5/FyTH/jR2wvLEPBsTLpBKlt8FguWiSHbrkIFh4YVqmRPcNNbhnJCX4VSWoMvgWl6fihZbvw0ncHsFNFSADA4p8KMe0/G6WunL7YcawMN7z5I3adKJfq3HDz7Oq9pzHngx247KV1svcwxqQI88wUiyzN/E8f6hdMGSxWRSVGPaoOBwIPtuNxTHzW7WThv+D6A5+9i9H+koUl5KBb13+1GBYOz0yq9rPDtzI2aslmj3tB6jhr9p4k2B1OHBfct3xd7lYwmQwwGQ3o5y5Z8POx4Iuy8dmoVdZhuLE6LOrHQl2DA4s2Fcq2l9vSE7dS73Ditne2YOg/VkUszZkxhrfXe2KOFv9UiCe/+FV6Li8c5/rvYMzLijb/uv64c0xnAMDQjpnSclGwWExGr9IVCRGrw6LdrVm5jJ/fBoNBusmLwmRQfktc0c/lflEmNEQSZYZSl9apMgtSgpdLyB2TI0z0fj1ZIWU9BeISCjifd/r06Th79iyefPJJFBUVoU+fPvjqq6+Qn58PACgqKpLVZOnUqRO++uor3H///XjllVfQrl07vPjii7j66quldcrKynDHHXeguLgYGRkZGDBgANasWYOhQ4cGunsyjmvEhnDMRkPE+wiJpFp4EzvX7EZ5QeJmP7F+QCioXfTFG4qaCZsxhrmfuGagi38qxL0Xd/P5GTe+tQlVNjumvrJemvF1aZ3q8z3V9Q5pP1qlWL3iikoqbLJeKpHGJRA8F/JIBwPzWIiW7ouweBOrszuatN/IzuPlKDhWBkBu2tUrC0Or0q0IFyw1Kv2M1PA1OxUtLMqmbA0OJtX0ENeViju6Tfx8IvLCqn24cViHoDL81Br2acewuP5rWVjeWHMIX+50xUnwG+FdY7pg0aZClNY04NDpainrZsexMikOKZyo3XA/2XYCgGs8Rg2XUI3it8tKtWLOhO4Y1KElhgkxR+mCeFb72hIilNbsq/Cn0sIi1l2xmo2w2Z2yYpsdW6VIv19TNq7kcZUAMLRTJrLTEyWPBeBd0E8thmVPsacMSY+cNNhq/EtkCKoAyd133427775b9bWFCxd6LRszZgy2bdvmvbKbF154AS+88EIwu+KTxkzELXSyXASL6Duuttm9rD3cJdQiWV/BIrqEaoRZKZ+9OJxMukiI6YP+BNRzv6RdSEfNVwnqszuckirnaZVJCSYkWUw4rRBOpTX1TSpYlBaWSMfWcAtLpvv4sJqNMBhcwdN1DQ7ZxTnS8H43gLpLKFQLC7eE+nIJcReivxVmfVnspA7JZqOXK7DBy8LiOsal4mDum1KKMJvfebwcF/VsI9uOw8lwx7tb0CkrBQ9f1ltjH91Bt+ZAXELqJ+iRs9Ve66YlJuDpq/vizve2ys5xf+OAQsWXW1V5IxeDbuvc515+q2R0ykrBkI6ZsJiNmHievG6XeLyo1SvinxFI5lcweIJutdOaOenCxDQxwYSKOrusYeDs8d0kwRxpt7QId9/nt0rGqzcOBIBGXELeWY180jDpvGxYzSb4G+0V172ElGpcSVO6gwBXIBZXzGpVYKWS0kn67Kda4Tixv09pTT2+23MKvR5ZjqWbXVYyMRU5mItZqtWMVikWr4sQF2NlNfVSlDw/iId0ypSte66JiyQpXQOR9B/X253Sb8SFq2gybsoLF6CIDRDul56g29D2j/u5faXeShYWf11Cit9PPDQ9tSSMYIoCAHZnIxYW90VbPJdTVMzdO46X4ds9JXhr3WHVLBWn01PUUmwboukS0mgOqIa4jkVle797f5vUdTec+KrSrLyRm4QYFv7bPX75eVh4y1Cf1kUe7M8LDIok6CSoG8NXaX7l76kULCIf3jUceZnJ0uSpKV1CXLBM6JUtWQ8TfLqEvPeZW6kDjc2Ma8FS28gFTC9XSyjwWama/11q2qWThUXNdCyKkJJKG95ccxj1Dice/HgnGGMyVexv2qhIqtUMo9HgVUeGizFeGwJwqW0AePH6AXjiivOk9zR1VUdlzEokXULi7yP6etVmLU2B2IG1Z9s06bEeFhbGGPa5S+TzytBq8Ju6r+aaIsrfT7xBihYW5f3f4WQ4phLD4hCCbgFPnzBA/fcRY01E8zqn8FwN6hqcsJiNaC/EmjQWw6IlWMTsG6eKqFSi1iVYb7jQTraYvGqmKIusSS4hxiRR6k8590/uHoF7L+qK+8Z5u7H5ueSrXYge8GNELe5NKcxSBcucsr8dv09Eg0uIu6nEQnzisaQUwokq16pgGh8CcS5Y/HEJNTVSwKBwYyqvaUBZTX3YYljEC+bd73tcda+tPoiNQtpgg4PJZvDBZD3wC0MbL8HiulBf+cp6adlTV/WV3nPTiI4Y6ra0lFY3bZEkfhzdPsqVAl/b4AipKFggcDdHYoJRFtjmSW1uWgsLn21NG5yLthneN9dQYlgqau2ocN9QOrfWrhXCj7GK2ga/6mrUur+zV24Y6N5HT1Vlvr9Ws1GaoYvwuhiAZ8aovCn1bpeOjm4XptpMuEqItRn21LdeQfe73enSPXPSZDeCfJX9AYTv2q5+TIoCTRQ1Wr2JtMos6ImYBfXmzEGy15TGCMkl5PQc7/7MzLNSrfjTpB7olu0tdnmBynBPhnw2P1QsE13PyswZLgL4RKUpLSz8XJEHhHvG4j0u77TmWpUYLX+Ia8HCT9QL3f1plOjlagkF7u+++rWNuP6NH1HX4MDk+Wsw4YU1Ur0PvWNYxInYgZIqjbVds01RFYvmcC2UF0EuyNoo6iKoZSMoUxG5UCtr4gZt/Dga18tTa+jRz3dFJKVYqxokP9Ff+k6fWkVvrjmEaa9vxF8+/jkg1x931w3Ol7vxLDq4hPi2Uywmnxe29i2S0CI5ATa7E1v9aJrJj2nxeJvzQQG2FZbKXEJ5mcl4f9YF+Oiu4dJ6YrVRKUvIyaT3cLLdx7vaTFgpULa52xpwSt0TlWzFOTN7QndcPTAXC28ZIlvOv2ubxnctusBkFhYNd8qxczVhF+Sb3BMjq9no1UtGWbNEDLqtDXJmrqSl+5oa7oqxUlqzah0W+bVSPH6UcWncYqm3K7i23oH3Nx3FancvMH+wCYHpHF8uIb7Pz6/cJwnE2npeGJEEiwSfGQ/s0ALXDfHuNRQNFhbxRrTx0Fn0fGQ5isrrcLrSJjVh0yvWRllgqrEbbr3dKVPyJ0prG53BKs2BfKagtLA0OJyNXhT5bKKp+vcArl5OPI5HvIH834+FUjntcFKtIVi4OfabX0/hZCN9VRrjbJUN//hqN346fA5LNh/DUiHNVwunk2HGfzdh9V5XGXvluSTVYdGY9fuDMjtKC7PJKFWb5dYJLQrP1ghp2J7v9Iufi3DVqxuw6ZCrJhI/jkd2zUJfdyNHJUqrjDiztKqkcnKUbgjlLJ+7PZRugVSrGc9N6yfVb+I05n4TJx3iOac8V68ZlAuDwXWTPeuj71eobC8sxSOf/QLA5WZWHtvKnmN8wvDwZ7uk10KtS8WvqaFUI/YHtWODo+xILQqY9CQNwcItq/bQazA9u2Ivej26HA8t24WbF2z2e3uSYJEFhGu7hMTYsrfd7TMk4UmCxYOnfbVZNQ8+GmJYEv2YKWTotJ8GRYGpxrIqXOWgPRc7u5PhoWW7sOIX9b5RgLe65u0HlP0mGhzyFMU5E7p7bytC5bOVfL2zCN/uPgXGGL755RQcTobzBDM/Z90B9SZ5elLp/o2UwZvijdBXF2x/2HxEXrjQH4F4vLRW1iRQOUvmFy1fwZWNwbOjWvoh2LmYVIsJ4Ww5cg5TX3W5IJMSTLJaJBxevVYM5tRynXjqsHg3uPPEGnh/lxUKi2Gp4gZdF+DF3Jdg2XDgDA4JlZLlLiH5uZosNGT91/I9YbMg7nCnwQOua4pSmCnhbmpxfKFaWFpEyMLCBZbaNVzMHFIeY0qXkNXkGi+fxLEQazDVNTjw8vcHFMv8u87WC25TjpjKrAyE/s0AT3HWVbtLpM8HSLDIqG3wzFTUAsyyVTp6RppEH1HuHK3yzIEiZglV2+w4//FvvNaZO6Wn9FgpWABg6ZZjuPO9rT72VT6ey92FjnJbym/2dqdTNrP8/cVdvbcVoWqUIjuOleF372/Dbe9swY7j5dKM/cKuWV4p8MobTzio1hAsM0fkC/sRWuDgEoVFRU3cK5n2+kbZc+UFNtPd++qsDwHRGNw14o8llN9oz2gUN6ytd+DGtzZJN5DnpvVDi2QLrhrQXrYeLy4pXi+0Sh/USy4h78BKteDI05U2jHz6Ozz1tbwX1TnFTdMz0QpNsGw9Woob3toks5bIg27l47KYjNKN9cOtx/H457/49fmBcuSsvD6WwWCQtTTwh1AtLFIMS21oTSobgycqKF3igPwm75UxpOESEntLhRJ4+/vF272WVdr8u57xHnFyUe8t1jnDu7TCd38cA8A1Iaisa5CsZv5M2EXiWrBw9ZxsMckuwq/PGIQ/TewuVQ1sStR8eBcJXWJHd28ti50IBalwHAPG/Hu17LXP7x2Jb/84BneO6SLdIOodDs0Ge1onuV1R14B3pG6TrnQJMcx6Z4v0XO2mwMWPcpvhZK9QLvxASRU2HXbN7nrkuAL3/nV1X+n1SNSs4J+RphAsNwztID3WqpTsD/tOVUpuHf4ZjQX02R1OWSwH4C1YuAvwdKUNjDHsKa7AZwUnGt2fXSfKMeudLXjss11SY0J/+mi1cgukb349he9V/PGVdQ2ycfHeJ89P76+6PaVQn9rf+1ohpTVLcQqiYPEOjtxxrExW84Rfk/Yo3FiBmss98ULyfVZLTxbPW696GQlGWbzfuz8eDdl6p4ZYF4Zzz0VdpSB7JZ1VOi+HKliyUi0wGQ1wOJnPLvKh8PmOk5IAVrrEAXnQqvImn56kCLp1vy66W0IJvF0p9NPjVPtZeFHMpOPIXEIqk3AxIP+2hVuw3G2lJwuLm6NnqyVT9/m5GcgWbpiTzsvBvRd3CzjgJxyIijkzxYL3Z12AZ6/thwcm98DSO4bh3Vt91xoIBL6dSptdNhOdM6E7+ua2kCrSSkF8KhYWjlo6IGPMKzOrVYrre1dmXNgdTNZLRA2PSyhyFpZfTnrKqX+7+xT2naqC1WzEqG4uETltSB4enOyyQr2z8ahX7Q/GGD4rOIFDp7WDmdXQEoD8e1ZaWAwGg3RzCeWmIu7nJHfXYz6DYoypzuLUmj8q4xB4Snq9w4mymgZcMn8t7ltS0Oisfc4HBVi1+xTe2XgUP7l7bPXTiCER4RaWc9X1uGXBZq84COXFXUyzH68yITilEGTzrhvg5basdzjhcDIpiF2sRCvVyxDM7GIRsL65GbhnbBcALpElno989tmYq0T6LA0Li5obQhRVyuuKxWTCX6b0xJ2jOwNwuR1CjY9S48gZb8ECALkatXbevlkeZPzQJb18FhL0B7PJKLmpT5T5rogeLH8QrBjKsg4A0CbNY3VRikfxNcAjbmU1mIIULGLvLMBz/FT5meLNj2lNl5CKN0O0Fv4kuKCVQq0x4lKwOJ0M1/5nIxocDPmtktEjOw0zh3fEpX3b4oXp/Zp692SI1TSvGtAeI7tmoVWqFXeP7YoLOqt3sw6Wliqm9U/uHuHljhFNzFonhRisxhjDd3tO4dXVB70umnymkNsyGf++xmOd8MdqYpZSY9Vv5owx/Hy8TJe6KHuKK/DW2kN4Z6OnBsXPx13ipWdOmuyCkypUKH79h0MAXJkVcz/5GTe8uQn3LSnAhBfW+P3Zzyzfg0F/X4lj57wvnFwwqt28+HcbSi0J/vuO7NpKsmRwq9pfPt6JQX9bhaJy+U1LbT+VHVetZpNkqSuptEmuiIUbjvi8CSoz0cZ0b40bLuigsbaHPkK3WMDbXafMVhInK89P74fz2qXLXlebdc8cni/1xQJcAcVivE9jLiGe4jq0UyY+v/dCXDPIkwjwtbt0PhB4yqfSJeRwMpTXNKjWpXlGsBAqb5IWs6sdwdxLeqGn26JYrHNncle1YPXfX8tF3zErBb8RXHdivZ9Q4DFM/mQ/BooY/2M2GqQq1SJiMURluNCADi2kx8rrtidTKPDrHmMM77v7SnF4/Je/laJVLSwm3xYWLfrmZjS+kkBcCpaz1fXSBefVGwfCYDAgMcGEV24YKAsAigZEC4spzE31lOa3zlkpGNihpZc7RrwA8hum0lzLBcvcT35Gp7lf4daFW/DvFXu9PlPc9rWD8yQ3nD9xKTx2R0vcfFZwEle8vB53/p8npsbucGL/qcqAAgYPlFRh8ry1+PuXu2XLuflemaUlWjuOnK1Gtc2OUf/6Hot/OiYFCDoC8Iu/tvogSmsasGD9Ea/XfN280qyuC9nJsuBvKlywWM0mrwvh0i3HUGWzY+GGI2hwOPHhlmM4UVaramFRlrEHPHECyhT2tftPN7o/6x68CNsemYB3bh3q1427fYsk/FGwgCgDh0UhrRRA6YkJuHqg/LqgDIQFXMfBD3++CDe7C8PVOxwyYSRetKWGb3bRwiLPGOnQKlmyZogFFGskC4t/nVOUKeQz396Efk9+I8uYevqq83H4qUtkwk55PRBvNLwfktLSFCpFZXUyS5N4M75rdBf0bpuuGs+SJwRI+xOE7Q/8Rh1MQczGEK9v3/9prKpFSIzNOlst34cOmckYnN8S+a2SseEv42SvWVWOLX9Rew/PwvPXxV0vXDM4jbmEAOB/914oez6lT45q2xZfxKVg4bOCNmlWnNcuMAUXafxpbqYXSmGi1dabXwA3Hjor3URHdsnCkacvlWaij33+CxxOhsU/aafAqvmezSqZP2op50DjDcr+606RW7PvNHYeL8cP+07j3yv2YsILa/DR1uOa+6WkQMhaAIAJveUuAmXQpyhYTAYD7lviHcDmL6KwUbNy+wrA5DeVr3cVeb3mLx7BYtQ0NdfbnViw/jD+/NHPGPn0d/ji55N+bTtJo1aMmF0kYne7WACXi8mf2BWR34/rJrl+1cYAAO0yEvHP35zv9V5lavGz07QtsfyC/MaaQxj6z2+l5bK0Zvc6ojWKj82ksh4XGwdKKqX4Ar9dQorS5+sPuEQz79B884iOuG5oB6/zPzHBKAu8FW802W6XRIlKQ9RQ4DfmtEQzZo/vhk/uHim9lpGcgK/uG4V7LvIOwO8tWMC6tvHdTNVfPC0d9K8aK9a+UdbT4Yi/h3ICZzAY8OFdw7H6T2O9zv1QXEJqLn4eu+avhYVbDa1aFhaNCsrn52bIMi07qtwfGiOo5ofRDg8IzImCLKDGEAWLP9kZenLtYHWhwA/Eeas8N5rMFNdNu2dOOn45WYGfj5erBlE+dEkvjO3RGmv2n5EFD3O4KGtwOJGeaEZFnR23u2eZSsyNdPwVLwqXv7xOvh+f7tIcn9fnCN97mzQrbhnZURaUppzRiSLDZndKqXpe+1fvaDTTQ3S3qFnYJJeQipXhsr5tMf/b/Th2rgaMsaAaeYoR/1pVNOsVY/zeHaR7yfk52HKkFMM0XJd87EqBsuHgWbXVZW6bYOO2LAoBoNy21nZzhRir3U9O9vm7cbeQeJP53dgusuJgPM37m19Pod7uKrMvBeeqZIfwY1ysPO23S4hbWDRuYMr4Io7BYEB6YoKURSTegLgICrUXlBJeuTi3ZTJmj/cuZaDFhN45mDe9Py7onKlb7GGK1NJB/+B5ft6ajAbNtPjG0DqfQ3EJqaUu8+NDrZinGlJHcw2riq9zV/ztWgfRxTw+BYv7JpCjoWyjCfEioVf6sj8M7ZiJ3w7LV31NecBNH5yHq9wm86euOh92pxOfFZzED/vkpv07x3SWxIdaOWxAnqrMYyW0LkAJUpZQ4yXHlQTSw4ZfsIZ1zsQ7tw71agWgtLCIbrwfD6nffAFXpdb2Fu2mfYCrbwzn9R8OYcOBsxjZNQt/caeX1/qwsPDAPCdzfUfBXBhlFhahDbzoUrPZnVDbcn6rFLx0/UBVyxCgbSE4V12Ps1U2tFJcsMQAVa1ZWmN4ytSrW1i0Lqa/GdAepyttuLBrVqMi89pBeXho2S7p+dUDc6VAbM70IXn42xe/AnDNXDPNFk+9FhV/v83uxH/XHca+U54gaF8NH0V4AS9NwaJhSQVcBcrUBIu5kXMvWLgLLd3HPqlhMhowVZGCHirc5RaObD/JrZdgCmoi4Qupj5gOFpaBHVpIlo79Jb6TIDjiNYOTrtKpXQ3xWq8lpH0Rny6hGLKwWIUfUK18s948ellvtEqx4G9T+2iuozzgZk/oJplPLWYjBnZoCUBeY6NHdprk2/eFOKPkF1itWjS+LCz1dqcsRVQNfyvk8gtWu4wkWM0mZKXKLSpK4TumexvpBOUX+86tU/Dx74bL1jvnRw0SZQDrzhPl+M8PByUrjq8YFvF3CrbJoHoMizzYuqrODrVrbnpiAkxGg+YF2ddM+KrXNmjui9loCPpc0GoJ0JhgSTAZcc9FXdEvr0Xjn6HYhpprNdVqlgQkN6E7FDEs/HMB4JNtJySBYzQAi2ZdgD7t5YHAje2Pze5UzeryVSBTdqMRvnN+7gUSi+UPPKNNWcm1KUhxB8/X+JnOGwjB1hnxB08GWhAWFvfx0SrFgp/+Og5L7hguudt+9aNy9/HSGkmMWWSCxfN7+sr8EeOm1DqZN0Z8CpZyl580FgRLYoQtLLde2AlbHh4v1RVRQxlA6VWvwb3PPBXTYjZixf2jZbn2WvCLuDir0bqxmX3EsKw/2HiV2f3CbNUXvP5AsvsCZjYZceMFHdAmzYpHLuuNKxQ1OExGA774/SjZsjHdW2NQfibmCXU9Gks3djgZHvx4p+pr3PJS66PZmz6CxeOP5r/7t3tKMPpf30vrLP+lWNWapawVocRXDMbRs96ZRmq+8UDRSvFV638SCqLVTSvAW3KxNcjrtZh8VAUFgD9O7IERKoUKtRCD5JW1NAZ0aIEp57fVfK8oHESLF99Hvdti8CKHysJoTYFkYQmDS6i4wjWZCrVejBqhxbB4rNpt0hNhMRulbKnTGkUXRd5ae1h6LJ5L4rXAYtIes2idTrYG/t3Ep2CpiB2XUFPEsDR2IVRe1NXSHwGPdSEQ8y4XIZV+CJYEH1lCSsuEWmbB4TPVYIw1etGVqskKWRn/+M35+Omh8bjtwk6q2Rp5mUmyDAceCDh1QHsMdKckNhbEJo5hXE95f5j5q/YBAOp8uIRMRoMkcoOtySB2XhWFgjKtd8fxcihprGWE8jhaMXu0n/sS/EU+QSOeo7EYlkARGyJqkagIhlXr3KsmoAJtxSFalZTujXduHepze6JZnhd5FPdRbwvLrhOu40ivdiOhkBqmoNu1+0/j1oWuopiFKiUARII5HkPp2MxdQmLROj6x4A0JfSHur/gbio99jUm8jpFLCK7ZTpE7zTPWBEuwwVl6411QSl2w8OJcgRx4XITwIkUJJoOmUPNVh+WXE3Lz5aD8ll7rbD1aik5zv0K3h77GAbd/dtcJ72BhXq8iEBOlwWCQWZR40T0ASHXPHhsrxCRezP7+G7mL7qhkYfFd9bSxxneNIXMJJQR2OWhslizOXId1zkSPnDR8do8nK0RptdHDCqIZdKvSsC0UurZJw3VD8mA1G3HbhepB41Ksgfs39FhYfAcoBnoz58Jo69FSr+y4xn4j8ZgXr5fhiGFpcDjxpbveTDQ0nuU3ar1jWB4LoKVBMNbERCHWLFD4sZgonAf82qJVJFSEX6t/O6yDzG0rHme+7mPi5/qbBScSd4Jl69FSHHJXUtQK/IwmxAPWFOa0Zn9R3hyVByC/EPPZVyA3en6Qc+tDoo8biJQCrbCwVNnsWOou2z6xdzaen9YPQzt66sTwG97b6z3my893uC6Ul720DvctKZDa2wOeC1agJ5AYzCgTLG5Tp5aFZcuRc3jgox1SevD4Xm3QNiNJ5k7iN3NeSVcrEFStQFkgiFUr/amJIBbQa+zGKmYdvHLDQACuQlH8eFJ2ypVcQgEKJxE1Abdm32kcKHG5B/VyCQGuAPTtj05AB0VTTI7SdM/PF/F8UutxFqhgEWNo5n/ryez7/k9jG33vVQNdgazJFpPsBqRWfiBUxCJ00/zM4Asn/Lrlb0l6fzhTZZM1m2wMXq06ECu10tUYCB6XkOe35tf72obGO0Dzz1Q2OxVdi75qbCWGaGGJuyyhBz76GYAJnbNSVMshRxuihSWSWUK+EG+OBoO3q8qfoEMt+IWQV2f1Zf6XsoQUJ8DBEk9syr+v6YcM92xtaKdM7DtViSl9crzqwxQq+pfsO1UpVRLmvXgCPYHECrNioG6qj7oGGw6cwQ1vbZIt65DpEgpTB7RHbsskXPOfjdJsp85HDAsgD7gMBi4SLGYjurRORcdWyV7N6f44oTueW+lyUWWlWqVCW40FToqdcHl8hMFgQMtkC0oqbdhxrAzthEwYtZLfgaIMuv3p8DnMfPsn6fVQtq3EYDD4LO5mUYhJ7ppsLIZFTcT4QnnzAIDz2qWjkx91LkZ0ycJ7tw316qZuktyx+llYeJB8p6yUqIgv5OdpKL24lCgrLF+p0oNK5Kmrzkfn1ilSFqY/hBLDwo9F8b7DRYTDydDgYLCYte9DWnFm4vbUuqBzxPf5WxhRJO4ES0mlDUZrstQXJdoRla45SlxC4sFnMRm9Yl6UB6vaBVMLLkJ4tdQMH4GbWllC3F0ypGNLSawArsyKeocTCzcc8drWhoNnZaZfPpv8+XiZVFdEq8CTFuWChUD8jlJ8CBZlKjggLzWeKMx2xP9aFhYtF4g/1NTb8cXPRbLtLLt7JM5W25DbMhlLNx/DoPyWaJNuxX/XH8a4ntmSaw1o3BLAa/Yo6domFSWVNty3pAAXdsuSjh+bw+OeChZecZOnNa9RfN96xbD4A/8tT5TWYt+pStUYFjXzeSATAEBdaL94/QC/38/7ZImEI4blhLsEvr/p2uGG11cqrdFPsPB4mOx0K96fdYE0GdGiRbIFf57U0+c6Sqx6uIQSvF1CgOt64+scEV3ISnY8NhH1dqdPi7tRuE6ShUVgZJesxleKApqycJwWSQrBosTLwhJIDIv7As0zjHzNAqXmh8JF0+5w4h23IFFeDMwmI8wmo2rp7pJKG9Yd8GQWzf1kJ5ISTJi9tEBa1rZFYILl6kG5eOm7AzJ3FOD5PtR842qzou6C69ITAOeQ/deysARrHj5yphpjn10tPedCsmWKRSrVfZOQpr75ofEwGw249EVPgb7GbqwPXdoL6UlmL/P/P39zPsY+uxr1DieKy+skwXLU7coNpbGdVWFhUc6e9XQJNbov7vPkL5+4MsFGdXNdk7RiWBJMBtw+qrNXX6PGUPsdRBdlMJjDYGHhPaTaBXiehYsW7mKYtQ0O1DU4dClIxycpKVYzurYJT0gCP+d5Yc9ACvBtLywD4Epr5iSYjDAbDbA7GWrrHT4nImo1WDj+uDIv6JSJt9cfxriebRqtd6RG3AqWYFKmmgK1Yk1NTZJFuIiqHJjKi35gLiH5e33FTfDv4+jZGlTb7EixmvHm2sPYerQUgOvgV0MZbN06zeXGuPO9rbLlolgBgLbpgc387rmoK7pnp0k3Io5kYVEJulVaQv56SU/0F2p/8JO4rsEJxlijjfC06o40xtxP5OnUynYESpQVWcVlWmSmWPDY5ed5Le+YlYK8zCQcO1crVT4FgCfddUgC6QOlRBnDUqH4DRpLxdYTpbuTV/yVdUwWvsO/T+2D6UMab/SoJEVhWr9puHpByEAwSSUFfB9XdQ0O3L+0AF/vKsZ947rh/gnaN0/uEmrfQj3mJ9KkWc3SjbqspgE5GaHfM3jMWTDWA38R7xnzVu3HXWO6+C22eKHLSxSp7kkJJlTa7DhbbfPprrOpZBkFwsTzcvDrE5NlnoVAiI47ZBhQnsTRSrRbWNRM1sqDzVclTSXK7fnqJyFe2O9bUoBHP9uFZ5bvAeDqLnzNIHW/r3KGOqiDdwaRGoHezBITTLi8Xzuv5oj8+6hUs7C4LSF3jO6M3U9Oxh2ju8he5999vcMp67arGXQbZBGpkkpPAOS86f0lq0pj6FXwizduFC0g/HsM1MIgwo+ZRZsKsbe4Esu2yzPCeuQEv+1A0SyIqBHDwisXB4pokRqU3xJzL+kV1HZEEvx0Cf10+By+3lUMAFKbCC1ORJmFxWAwSNlK51SaXQZDFa/pFIaCcZyLFSUQLntpnSTyGWM+x8IFfDuFW45fq25esNnnZ/tyCflLkiX46r9xK1jCecDoia8smaZCnmqtZmGR73NAMSyK7XXUyLBQrrtq9ym8u/Go9Pzf1/TTdB20SU+UrCx/ntTDZ5E8Eb1KaKf6cAlxS0hOeqKqCBG/e7FjsGbQbZAWFvE3axNAcPqjl/XG0E6ZeGCyd92bwD6fBzy6vqMGh1O60N57cbegt8vjJE6W12HSvDVer4cihgJFK6BcrdItAF2SBK4a2F4X14a/QbfKOiO+io9JFhYfQZmRhruP735/a0iWPQ4/58NpYRncMROr5oyRnh8oqcJx93H/xppDGPi3lej4ly+xSuiHxuFWXy2r+OlKm+SuV0OP4o6hELeCJZwHjJ6IpjW1AmlNgXgjVfP5K82BgXzXQxTxHh0DbC/OaazGzqf3jMQ394/GPRd1RX93ITcl/7qmb1Cf3Ri+soTqhawcNaxmo9Sbhxfms5iNmtY3vh3uJvOH1XtLZB2qA7lR9strgQ/uHI67x3p31A0EfsF83F2z4u9udxAAr9YIgdBYumxPP8WrHmhd1MUYFtHimBVEMzjO3Ck9MapbFq4OINvEF560Zt838f2n5P1nyn0EsHJBGkzTu3DRyn2sHTlbE9A5pIWnREJ47z9d26Riw18ulp7zqtovCmntz36zV/aeugaHNLHxZRU/66OliK8YlkgQt4IlFmNY9K4qGSyyoFudY1jyMpOx52+T0TI5AW0zEr1MkyLd2qTiUo2y4o0FZuZkJErBrMM6tVLtheNP2mcw+BYsviuuGgwG6fvnFhZf5b35MbNg/RG/909p9m2K9P+f3ZVzz1bXo6ymHu8I1rNQzM1Tzm+LW0Z2VH1t8nk5ujei84VWeqfo6hS757YKQajdOaYL3rvtAt06GXsKx2lPosR6SJxbFm7G8VJvtxBjTEr5jYYqtxyxkKBaJ+NA4YH9wfTJCZR2LZKkY6zBwWCzO2Qd7A+UVMkyicQyDKkKQfWniZ7YoxofrQqk0gNhaDngD3EpWMxGQ0SzAUJBvID6KrgTSZIacwmFUIcFcLk9Vv/pIqy4f7TPuB2DwYDnpvXzWh6ICwNwWYzUYprCdeH0J+jWZ4Mwt4WLm9t9CRax43YwaY7jerbxisGJBGLROLERoh7iae6UXpg3vT9WzB6NPX+bLFlVtLqTh4uhGkHh4jHfMycNvdumY2Lv7IDrr4QTf9KaNx06i7oGJ6xmIy7q4UmNfvrrPV7rVtc7wDcVDY0POaIbyNeN2l9+OnwOQGhWwkCQXMJ2l0vVyVzHV3qiGXYnw4+HzknrVgnuKuWE796Lu6Gbu72IWt8wDrmEwkByCEE9TYmeVSVDQTQXqgfdym+gqdbAL0AZyQl+NUBLTDB5Bfm+9ttBAX+eWrxIuOKcuIBTWlgOlFThXLVrlulLUPOYG17i21dE/ZQ+OdLnHTnju2+JGsrGjpHimas97jixMuiyu0eEvG2L2YipA9qjR04aEhNMWHrncHx+70hcqMjmCjfDO7fCY5f39sraEWNYzCYjvvzDhXhj5uCI7ltjeJofagsW3gbgqoG5ENdyqsSCcOuKxWRsspudGqJ4fGb5HixcfzjoWBab3SHVYbnJj871esBFbnltA9YfcGUAZaZYcGlf13n9mRB0/ur3BwBoV+Dm10Ot3krHS2tw0H2ukmDRkf5+ZoUQ6nQT6gecVYk45wqeE6iFJVBaJHlmK7ktk1T7BgXCVQPaY0LvbLRvkYS3Zg5GVqoF79w6NNTdlEgRmqrxGeqRM9WY8MIP2F3kKqTmKy1wSh+5G0xZeVbEYDBIcUBqpvjGCLevXYsr+7f3WmY0hKf/V0ZSAvrmttB9u41hMBhwy8hOmKz4PZUVraNxcsVvhGoWFsYYjp2rwYpfXNlBN17QQWZNUrMU8arH6UnmqBrvA0LRtoOnq/H4/37F5ztOBrUt0eWiVgsqHHBr9+3vbsGfPtwBAMhMtmBYZ9fv8cn2E6i22fHwpzvxoaLPlBI+qavVyDh84n+uOLOWyQlBxx6GSlwKltERnkmFyq0jO6Fbm9Qmm+0qSbKYJDO6VpyHmAYb7gDnPu0zpMfBtqW/c3RnAMDfrjwPz0/vjzdnDobBYMD43tnY/NB4jOnuXe0zWMTv47s9JQCAQ2eqIE7cfLVgv2FoB3TP9hT+UtZ5UcLNz76i+znlitLhKU2UTSd2muZ8fu+FXnV64gGlODXFwBh9ZQk9vXwPRv3reziZy4XXp30GfjssH4PdEwm11OY/um+mZ3wEdDYFvdul48YL5LVvvnRXfw4UbkVKs5ojVqJCzQLeMiVBVrX7jve24P9+LJSePzhZvbKu1FNIYWFhjOGTbcex0p119M6tQ/0ug6A3sZFKEyBtYqBLs8ijl/du6l3w4qXrB2DdgTOqM2FAflMOVkT4y8zh+Vi123WyqBWy84dbR3bCFf3bqda60HvGJ5pLNx85hwm9s6X6DBxf5a+NRgM+/t0IfL2zGHYnk2ZLWvDsEl83g73FlXj6691SGwJOchNm01nNRtjdF8curVNkwjSeUJrPHVHi+vWFJ4bFe19f/+GQ9JiniacnJuC+8d0w478/YVthGX45WY7z2nl+T25ZjEaU14TiCleNogc+2oGfDp/D3WO7YndxBe69qKvUE0sNXuMkkjE6atasuganTLBwVxEAPH3V+bhuqHpxQm5tVcbyfFZwEnM+cAnOLq1TcH4TnqfxKVhioOlhtNMtO81nt2txJpwS5owssXaG2gXUH4xGQ9CFuQLFYDDgrjFd8J8fDkoF3ZQBuI31tElLTMC0If51tG0lCRZ1CwtjDPcvLcCvipvG8M6tIprmq8RiNkrF8VqlxO85q8x64t3koxmzVOnWdzyHeG6Kbrc31hzC/Otc/YwOnfY0K33ksuibnGWny4+905U2VNns+GCLy4XywMc/AwB2Hi/HR7/TjrH6z+qD4dtJDZTXEaMBuKJfO68xAcD1Q/M0xQrgcQnVKFxCO0+4MvqsZiPenzWsSV160W+bDIJY6NIc64iuhHCb8cVZTaVK5k00wqvmcn9wlS18PW24S6ik0luwbCssRae5X3mJFQBYfMcw3dJgg0G8kWdHQffecKG0sPRuG7nidcHiyyUkBquLPYsykhLwpjt4+KAgUl4VbuS3aqScNyXKY6+ovA4XC322OFuOlmpmTS3+qRDL3TE9vEBeJBCvI5f2bYsdj03ErRd2QrLFjNnj5QUYG+ttxH/XHxRW2GJ3o9oHJ/ds8i7bcSdYTEZDWAL3CDnTh+TBYjLiNwPUXUbhoiGIlupNgeQPdtctULqE9CwS2NNdbn7NvtNSN1bOancMDQD0ae9ar32LJHzsY6YYKcTYjt9fHFohumhGHOdNw/NxtUZLiWjCV1ozj8Ua0KGFV0+atu4b2q4TFdh40OWK4MXGRnXLiqqAW47a/UJN/APA/pJK1eWLNnliRCJ5TRRdQi2SEmQVrGeP745v7h8tPdeqC8Th4nTT4XOy372o3CXA2kbBpCLuXEJv3zw4IkV7mjtX9m+Py/q2i3j/o/ooqVXTGMoANqVLKFtHUT2iSyu0TE5AaU0DDpRUyWJBeIpi/7wWWHb3COw7VYWubVKjom+VmP6q5/cRbYiWpOsv6BBV9Va04MXtGhTxNnUNnuJkC28Z6mWhayO4In73/lYUPDpREtGT++SEc5eDRqtVwA0XdMCPh86i2mZH24wkFBwrw/bCMmmCIMJ1WP+8Fng0gm4v0SWkVleqe3Yanru2HwqOlWGcogeRkot6tJGEV029XRI/pypc4i0arKDRf+YEyKB83wGKhH40xU0vWtoXNIan67LcJfSbAe3x0V3Ddb1BG40GdJBSm+XmaO6PHtujNQwGA3rkpEWFWAHkZdzTw5wa35SILqFYECuAp9Kt0sLC05ONBlc2jBIxFomvy8+BaOybBrgCht+5dSj+PrWPbPnlfdth+X2jsfpPF2Fk11YAgKWbj6nWaeEl+R+c3DOiGTTi8aSVSn31oFz8bWqfRl3343u1ka4N1YJFmGcWRipV2xexcfYQhJtYcQklJshrGvBiTf1yMzC4o/6iOs89S/x4m7zWQl19+LvHBkuFYHWKRleBXljNRvTITkP7FknokKnd7DOa4IH0FXUNePHb/Xj+m71gjEmB3a1SrartMUQxzGuz8HLuTRkv1RhjurfGb4flyzJgurRJgcVsRJLFhGsH5cFgAAqOleGwImj6QEmVVFAt0m0HLGbP9x1qfInBYJBiE/n1ijGGanfWULiTK/yBBAsRU0RL+4LGSFS4hM5UulKOfaVFhkK+u+v1qt2nZLNi7hLyVd6fCC8GgwFf/uFCrP7z2JixsLTLSEKa1YwGB8PzK/fhxe8O4NCZapxyp/yqZaFw/vEbl6WCi+Q6dzl3XxWbo4XP7hmJm4bn464xXWRZhR2zUtDPnQV18XM/SK4TxhjGP/+DtB4Pto8U4vHUrkXoVltlp/maeocUsxQNDYWj/wgiCIH6GKhhAXgEAjeHH3NXoQ1XlP3to1yF8RgDzlZ7Aga5SyipiSra+oNa8at4w2wyxoxYAVxuxk6t5UUjdx4vl4JRfZUI4HWZ+LHP/8eCaDYaDXjiyj74yxTv4mpiCvdfl+0E4HF7cSJtYREtWr4ayfpLikKw8P9GQ3T8frFzBhFEDMFP7kNnqvHOhiMocqcGZoepFkyLZItUf+hUuUewRLNLiNMUZfOJxrntwk6y57uLKlBSwQWLtoUlUZEhV9fEHX71QlmO/v6lBV4pzGpNVsOJWFW4tQ7WWy5YKt1ChbuGUizR0VIheqddBBHDiOZv3sQQkGdR6E12eiJKKm04VVGH8+Hyxdc0uC440TA7UrLg5iF4c+0h/Ouavo2vTEQcZVbPop8KJWuJr9k8P9ZsCgtLLLiEfKG0ji7bfkLWdRyAalxPOCkVLDx61MNSuoR48G20ZN7G9hFENBteu3EgUiwmvBVlXW21UHP9JCYYwxp4yDOPjglNEHkMjVq36qbmop5tsOj2YchtGRuBqM0NZYXeyjq7FEPWy0fxOy5MlC6haA669YcsFQvGakWRtUgzd0pPtG+RhFdvHKjL9nhg7VtrD0sVf8XlTQ0JFiImmHJ+W/z8+CSM753d1LviF2mJCbh6oLxA2P/uvTCsn9k/z2VV2XDQ0zukloJuiRBYNWcMHrqkl2yZweApQqiGMkOuzh79WUL+cEGnTPx2WIcmaxiqxuCOmVj/l4u9CvgFy7ieruvrr0UVePSzXZKlJRoCbgESLEQMES31Q/ylR46nbPmILq3QtU2qj7VDZ0RXV1fnHcfKAACnKuqkRm6RDgYk4oOubVIxa1QnTD7P5R66uGcbvHLDQLTN0HYJiRlyi38qRD0XLEE2Lo0WjEYD/j71fPzy5GR8fd8or9fvH9+9CfZKX6YNycNj7ma8X+8qFiws0SFYomMvCCIOMQpBaq/PGBT2oLUuWS5BVFJpQ7XNjm93l8DJXLVfeNozQQSKwWDAa78diLPV9apuESXcJVRRZ8fcT3ZKy6PRLRksPbLTkGAySC6yVXNGo3NWeCckkeLGC/Lx1Fd7UO9wYvbSAgDRI1hiW/ISRBQjWoTEHh/hIiM5AS2TXZ9z9GyN1PdkSMfMqIjwJ2IXg8Hgl1gBtF0/yVGcWh8oRqMBi24fhgSTAZPPy0HXNmkRD7gNFxazEXeN6SxbRi4hgohzrh2ch5FdW+HJK8+L2Gfy7rm7iypwoMTVMbdbdnzM/IjYoGWyBV3cNVzSEs3okZ2GhbcMaeK90p8hHTOx9oGLMe+6/k29K7pz/4TuUgNMIHqCbqNDNhFEHJJqNeP9WcMi+pkD81tiy9FSbDlaKgmWxtrKE4SemIwGfH7vhdh5ohyD8lvGVMG8QAlXIcimxmAwoEWyRWrFQC4hgiB0Z3B+SwDA6r0lUrG6cAf7EoSSFKsZwzq3imuxEu9w9zIApEaJO4+OJoKIIwa5BQsXK20zEilDiCCIgBG7TpOFhSAI3WmVakVvoajXLSM7Nt3OEAQRs2QmewQLBd0SBBEWZg7PB+CKJbh2UF4T7w1BELFI9xxP7FtaYnQIlujYC4IgdGP6kDwYDK5icaJZlyAIwl9uHtERq/eWINVqxvAurZp6dwAABsYYa+qd0IOKigpkZGSgvLwc6enaZaMJgiAIgoge/L1/k0uIIAiCIIiohwQLQRAEQRBRDwkWgiAIgiCiHhIsBEEQBEFEPSRYCIIgCIKIekiwEARBEAQR9ZBgIQiCIAgi6iHBQhAEQRBE1EOChSAIgiCIqIcEC0EQBEEQUQ8JFoIgCIIgoh4SLARBEARBRD0kWAiCIAiCiHpIsBAEQRAEEfWYm3oH9IIxBsDVppogCIIgiNiA37f5fVyLuBEslZWVAIC8vLwm3hOCIAiCIAKlsrISGRkZmq8bWGOSJkZwOp04efIk0tLSYDAYNNerqKhAXl4ejh07hvT09LDu05AhQ7B58+awbT+exgLQeEKBxhMY8TQWgMYTCjSewAjHWBhjqKysRLt27WA0akeqxI2FxWg0Ijc31+/109PTw37gmEymsH8GEF9jAWg8wUDjCY54GgtA4wkGGk9w6D0WX5YVDgXdhpF77rmnqXdBN+JpLACNJ9qJp/HE01gAGk+0E2/jEYkbl5C/VFRUICMjA+Xl5RFT1eEinsYC0HiinXgaTzyNBaDxRDvxNJ6mHEuzs7BYrVY89thjsFqtTb0rIRNPYwFoPNFOPI0nnsYC0HiinXgaT1OOpdlZWAiCIAiCiD2anYWFIAiCIIjYgwQLQRAEQRBRDwkWgiAIgiCiHhIsBEEQBEFEPTEnWNasWYPLL78c7dq1g8FgwKeffip7/dSpU7j55pvRrl07JCcnY/Lkydi/f7/0+rlz5/D73/8ePXr0QHJyMjp06IA//OEPKC8vl21n27ZtmDBhAlq0aIFWrVrhjjvuQFVVVdSNBwDuvPNOdOnSBUlJSWjdujWuvPJK7NmzJ2bHw2GMYcqUKarbiaXxjB07FgaDQfZ33XXXRXw8ev02GzduxMUXX4yUlBS0aNECY8eORW1tbUTHosd4jhw54vW78L8PP/ww5sYDAMXFxZgxYwZycnKQkpKCgQMH4qOPPpKtE0vjOXjwIH7zm9+gdevWSE9Px7Rp03Dq1KmIj+epp57CkCFDkJaWhjZt2mDq1KnYu3evbB3GGB5//HG0a9cOSUlJGDt2LH755RfZOjabDb///e+RlZWFlJQUXHHFFTh+/HjMjueNN97A2LFjkZ6eDoPBgLKyMq/P0nM8MSdYqqur0a9fP7z88sterzHGMHXqVBw6dAifffYZtm/fjvz8fIwfPx7V1dUAgJMnT+LkyZN49tlnsXPnTixcuBDLly/HbbfdJm3n5MmTGD9+PLp27YpNmzZh+fLl+OWXX3DzzTdH3XgAYNCgQViwYAF2796NFStWgDGGiRMnwuFwxOR4OPPmzVNtsxCL47n99ttRVFQk/b3++usRH48eY9m4cSMmT56MiRMn4qeffsLmzZtx7733SuW0Y+m3ycvLk/0mRUVFeOKJJ5CSkoIpU6bE3HgAYMaMGdi7dy8+//xz7Ny5E1dddRWmT5+O7du3x9x4qqurMXHiRBgMBnz33XdYv3496uvrcfnll8PpdEZ0PD/88APuuece/Pjjj1i5ciXsdjsmTpwo++7/9a9/4fnnn8fLL7+MzZs3IycnBxMmTJD63AHA7NmzsWzZMixZsgTr1q1DVVUVLrvssohfq/UaT01NDSZPnoy//vWvqp+j+3hYDAOALVu2THq+d+9eBoDt2rVLWma321lmZiZ78803NbfzwQcfMIvFwhoaGhhjjL3++uusTZs2zOFwSOts376dAWD79+/XfyBu9BrPjh07GAB24MABxlhsjqegoIDl5uayoqIir+3E2njGjBnD7rvvPs3tNsV4gh3LBRdcwB5++GHN7cbab6Okf//+7NZbb5Wex9p4UlJS2LvvvivbVmZmJnvrrbcYY7E1nhUrVjCj0cjKy8uldc6dO8cAsJUrVzbpeEpKShgA9sMPPzDGGHM6nSwnJ4c9/fTT0jp1dXUsIyOD/ec//2GMMVZWVsYSEhLYkiVLpHVOnDjBjEYjW758ecyNR+T7779nAFhpaalsud7jiTkLiy9sNhsAIDExUVpmMplgsViwbt06zffxin1ms1najsVikTVhSkpKAgCf29GbYMZTXV2NBQsWoFOnTlLn6lgbT01NDa6//nq8/PLLyMnJUd1OLI0HAN5//31kZWXhvPPOw5/+9CfZLCUaxuPPWEpKSrBp0ya0adMGI0aMQHZ2NsaMGSPbx2gYC98PILBzZ+vWrSgoKJBZW2NtPBdeeCGWLl2Kc+fOwel0YsmSJbDZbBg7dqy0nVgZj81mg8FgkBUoS0xMhNFolK3TFOPhIQSZmZkAgMOHD6O4uBgTJ06U1rFarRgzZgw2bNgAwHV8NTQ0yNZp164d+vTpI60TS+PxB73HE1eCpWfPnsjPz8fcuXNRWlqK+vp6PP300yguLkZRUZHqe86ePYu//e1vuPPOO6VlF198MYqLi/Hvf/8b9fX1KC0tlUxeWtsJB4GM59VXX0VqaipSU1OxfPlyrFy5EhaLJSbHc//992PEiBG48sorVbcTa+O58cYbsXjxYqxevRqPPPIIPv74Y1x11VVRNR5/xnLo0CEAwOOPP47bb78dy5cvx8CBAzFu3Dgp9iAaxuLveJT897//Ra9evTBixAhpWayNZ+nSpbDb7WjVqhWsVivuvPNOLFu2DF26dIm58QwbNgwpKSl48MEHUVNTg+rqavz5z3+G0+mU1mmK8TDGMGfOHFx44YXo06cPAFfsEABkZ2fL1s3OzpZeKy4uhsViQcuWLTXXiaXx+IPe44krwZKQkICPP/4Y+/btQ2ZmJpKTk7F69WpMmTIFJpPJa/2Kigpceuml6N27Nx577DFp+XnnnYd33nkHzz33HJKTk5GTk4POnTsjOztbdTvRMJ4bb7wR27dvxw8//IBu3bph2rRpqKuri7nxfP755/juu+8wb948ze3E0ngAV/zK+PHj0adPH1x33XX46KOPsGrVKmzbti1qxuPPWHjcwJ133olbbrkFAwYMwAsvvIAePXrg7bffjpqx+DsekdraWixatEhmXQFibzwPP/wwSktLsWrVKmzZsgVz5szBtddei507d8bceFq3bo0PP/wQ//vf/5Camir1rxk4cKC0TlOM595778XPP/+MxYsXe72mjLljjKnG4WmtEw/jEdF9PAE7kaIIKPyiImVlZaykpIQxxtjQoUPZ3XffLXu9oqKCDR8+nI0bN47V1tZqfkZxcTGrrKxkVVVVzGg0sg8++EC3/VcSynhEbDYbS05OZosWLfJ6LdrHc9999zGDwcBMJpP0B4AZjUY2ZswYr+1E+3jUcDqdXr5sTqTGE8xYDh06xACw9957T7b+tGnT2A033OC1nVj6bd59912WkJAgradGtI/nwIEDXnEhjDE2btw4duedd3ptJ9rHI3L69GkpPiI7O5v961//8lonEuO59957WW5uLjt06JBs+cGDBxkAtm3bNtnyK664gs2cOZMxxti3337LALBz587J1unbty979NFHvT4r2scjohXDIqLHeOJWsHD27dvHjEYjW7FihbSsvLycDRs2jI0ZM4ZVV1f79Vn//e9/WXJyss8fJFSCHY8Sm83GkpKS2IIFCzTXidbxFBUVsZ07d8r+ALD58+d7nVQi0ToeNfiYeICbGuEeTzBjcTqdrF27dl5Bt/3792dz587V3E4s/DZjxoxhV199tV+fFa3j+fnnnxkA9uuvv8rWmzhxIrv99ts1txOt41Hj22+/ZQaDge3Zs0dznXCMx+l0snvuuYe1a9eO7du3T/X1nJwc9swzz0jLbDabatDt0qVLpXVOnjwpC7qNpfGI+CNYOKGMJ+YES2VlJdu+fbsUafz888+z7du3s6NHjzLGXBk/33//PTt48CD79NNPWX5+Prvqqquk91dUVLALLriAnX/++ezAgQOsqKhI+rPb7dJ6L730Etu6dSvbu3cve/nll1lSUhKbP39+1I3n4MGD7J///CfbsmULO3r0KNuwYQO78sorWWZmJjt16lTMjUcNtYtdrIznwIED7IknnmCbN29mhw8fZl9++SXr2bMnGzBgQMSPNz1+mxdeeIGlp6ezDz/8kO3fv589/PDDLDExUcpIi9RY9BoPY4zt37+fGQwG9vXXX6t+TqyMp76+nnXt2pWNGjWKbdq0iR04cIA9++yzzGAwsC+//DLmxsMYY2+//TbbuHEjO3DgAHvvvfdYZmYmmzNnjmydSIznd7/7HcvIyGCrV6+W3TNqamqkdZ5++mmWkZHBPvnkE7Zz5052/fXXs7Zt27KKigppnbvuuovl5uayVatWsW3btrGLL76Y9evXL+LXAr3GU1RUxLZv387efPNNBoCtWbOGbd++nZ09ezYs44k5wcKVnPLvpptuYowxNn/+fJabm8sSEhJYhw4d2MMPP8xsNluj7wfADh8+LK03Y8YMlpmZySwWC+vbt69XqmC0jOfEiRNsypQprE2bNiwhIYHl5uayG264wWsGEivjUUNNsMTKeAoLC9no0aOlfe3SpQv7wx/+IDuhIzUevX6bp556iuXm5rLk5GQ2fPhwtnbt2oiPRc/xzJ07l+Xm5spSL2N1PPv27WNXXXUVa9OmDUtOTlbd31gaz4MPPsiys7NZQkIC69atG3vuueeY0+mM+Hi07hmiFdvpdLLHHnuM5eTkMKvVykaPHs127twp205tbS279957WWZmJktKSmKXXXYZKywsjNnxPPbYY41uR8/xGNw7TxAEQRAEEbXEVZYQQRAEQRDxCQkWgiAIgiCiHhIsBEEQBEFEPSRYCIIgCIKIekiwEARBEAQR9ZBgIQiCIAgi6iHBQhAEQRBE1EOChSAIgiCIqIcEC0EQBEEQUQ8JFoIgCIIgoh4SLARBEARBRD0kWAiCCIh3330XrVq1gs1mky2/+uqrMXPmTBiNRmzZskX22ksvvYT8/HwwxtC1a1c8++yzstd37doFo9GIgwcP4tZbb8Vll10me91utyMnJwdvv/02AGDs2LH4/e9/j9mzZ6Nly5bIzs7GG2+8gerqatxyyy1IS0tDly5d8PXXX0vbWL16NQwGA1asWIEBAwYgKSkJF198MUpKSvD111+jV69eSE9Px/XXX4+amho9vzKCIHSABAtBEAFx7bXXwuFw4PPPP5eWnTlzBl988QVuueUWjB8/HgsWLJC9Z8GCBbj55pthMBhw6623er3+9ttvY9SoUejSpQtmzZqF5cuXo6ioSHr9q6++QlVVFaZNmyYte+edd5CVlYWffvoJv//97/G73/0O1157LUaMGIFt27Zh0qRJmDFjhpf4ePzxx/Hyyy9jw4YNOHbsGKZNm4Z58+Zh0aJF+PLLL7Fy5Uq89NJLen5lBEHoQdB9ngmCaLb87ne/Y1OmTJGez5s3j3Xu3Jk5nU62dOlS1rJlS1ZXV8cYY6ygoIAZDAZ2+PBhxhhjJ0+eZCaTiW3atIkxxlh9fT1r3bo1W7hwobS93r17s2eeeUZ6PnXqVHbzzTdLz8eMGcMuvPBC6bndbmcpKSlsxowZ0rKioiIGgG3cuJExxtj333/PALBVq1ZJ6zz11FMMADt48KC07M4772STJk0K6fshCEJ/yMJCEETA3H777fjmm29w4sQJAHILytSpU2E2m7Fs2TIALuvJRRddhI4dOwIA2rZti0svvVRy73zxxReoq6vDtddeK21/1qxZkhWmpKQEX375JW699VbZPvTt21d6bDKZ0KpVK5x//vnSsuzsbOn9Wu/Lzs5GcnIyOnfuLFumfA9BEE0PCRaCIAJmwIAB6NevH959911s27YNO3fuxM033wwAsFgsmDFjBhYsWID6+nosWrTIS2zMmjULS5YsQW1tLRYsWIDp06cjOTlZen3mzJk4dOgQNm7ciP/7v/9Dx44dMWrUKNk2EhISZM8NBoNsmcFgAAA4nU7N9ynfw5cp30MQRNNjbuodIAgiNpk1axZeeOEFnDhxAuPHj0deXp7stT59+uDVV19FQ0MDrrrqKtl7L7nkEqSkpOC1117D119/jTVr1sheb9WqFaZOnYoFCxZg48aNuOWWWyIyJoIgoheysBAEERQ33ngjTpw4gTfffNPLgtKrVy8MGzYMDz74IK6//nokJSXJXjeZTLj55psxd+5cdO3aFcOHD/fa/qxZs/DOO+9g9+7duOmmm8I6FoIgoh8SLARBBEV6ejquvvpqpKamYurUqV6v33bbbaivr/cSM/6+Pn78eLRt2xaTJk1Cu3bt9Nx1giBiEANjjDX1ThAEEZtMmDABvXr1wosvvuj12j/+8Q8sWbIEO3fuVH3v+vXrMXbsWBw/flwKkBWpqalBu3bt8Pbbb3u5lAiCaH5QDAtBEAFz7tw5fPPNN/juu+/w8ssvy16rqqrC7t278dJLL+Fvf/ub13ttNhuOHTuGRx55BNOmTfMSK06nE8XFxXjuueeQkZGBK664IqxjIQgiNiDBQhBEwAwcOBClpaV45pln0KNHD9lr9957LxYvXoypU6equnsWL16M2267Df3798d7773n9XphYSE6deqE3NxcLFy4EGYzXaYIgiCXEEEQBEEQMQAF3RIEQRAEEfWQYCEIgiAIIuohwUIQBEEQRNRDgoUgCIIgiKiHBAtBEARBEFEPCRaCIAiCIKIeEiwEQRAEQUQ9JFgIgiAIgoh6/h952Rjsh6ze+wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[['volatility']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1151, 1036, 86.33333333333333, 9.583333333333334)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of the data used to train in a time-series split\n",
    "training_percentage = 0.9\n",
    "\n",
    "# number of samples, number of training samples, number of years in training set, number of years in testing set\n",
    "data.shape[0], round(data.shape[0]* training_percentage), round(data.shape[0]* training_percentage) / 12, (data.shape[0] - round(data.shape[0]* training_percentage)) / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data sample into (X_{t-1}, Y_t)\n",
    "X = data.drop(columns=['volatility'])\n",
    "Y = data[['volatility']]\n",
    "\n",
    "# split (X_{t-1}, Y_t) into time-series training and testing sets\n",
    "X_train = X.iloc[0: round(data.shape[0]* training_percentage)]\n",
    "X_test = X.iloc[round(data.shape[0]* training_percentage):]\n",
    "\n",
    "Y_train = Y.iloc[0: round(data.shape[0]* training_percentage)]\n",
    "Y_test = Y.iloc[round(data.shape[0]* training_percentage):]\n",
    "\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "\n",
    "X_sub_train = X_train.iloc[0: round(X_train.shape[0]* training_percentage)]\n",
    "Y_sub_train = Y_train.iloc[0: round(X_train.shape[0]* training_percentage)]\n",
    "\n",
    "X_validation = X_train.iloc[round(X_train.shape[0]* training_percentage):]\n",
    "Y_validation = Y_train.iloc[round(X_train.shape[0]* training_percentage):]\n",
    "\n",
    "\n",
    "\n",
    "## NOTE: in practice, you would do rolling predictions where you build a sequence of models, rather than just a single model at a single time-series split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Lots of Models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First one: regular old linear regression! Cuz why build a fancy model if a small one works well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 820us/step - loss: 4.0240 - mae: 1.9772\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.7825 - mae: 1.6385\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - loss: 1.8578 - mae: 1.3320\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1859 - mae: 1.0551 \n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step - loss: 0.7336 - mae: 0.8183\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886us/step - loss: 0.4418 - mae: 0.6201\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - loss: 0.2605 - mae: 0.4601\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step - loss: 0.1546 - mae: 0.3446\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - loss: 0.0968 - mae: 0.2684\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 754us/step - loss: 0.0668 - mae: 0.2226\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 0.0520 - mae: 0.1955\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.0452 - mae: 0.1790\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - loss: 0.0420 - mae: 0.1687\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - loss: 0.0405 - mae: 0.1626\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 0.0397 - mae: 0.1593\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - loss: 0.0392 - mae: 0.1571\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - loss: 0.0387 - mae: 0.1555\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - loss: 0.0383 - mae: 0.1546\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.0375 - mae: 0.1527\n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 0.0371 - mae: 0.1515\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.0367 - mae: 0.1505\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.0361 - mae: 0.1495\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 0.0356 - mae: 0.1483\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.0351 - mae: 0.1471\n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 0.0346 - mae: 0.1460\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 0.0340 - mae: 0.1448\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - loss: 0.0335 - mae: 0.1439\n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0329 - mae: 0.1423\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 719us/step - loss: 0.0322 - mae: 0.1409\n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - loss: 0.0316 - mae: 0.1396\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 0.0311 - mae: 0.1385\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0306 - mae: 0.1371\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0299 - mae: 0.1358\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 0.0294 - mae: 0.1344\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - loss: 0.0288 - mae: 0.1331\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0282 - mae: 0.1317\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 0.0276 - mae: 0.1303\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 890us/step - loss: 0.0271 - mae: 0.1289\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 758us/step - loss: 0.0264 - mae: 0.1273\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 948us/step - loss: 0.0259 - mae: 0.1260\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 0.0253 - mae: 0.1246\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 0.0247 - mae: 0.1231\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - loss: 0.0242 - mae: 0.1217\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - loss: 0.0235 - mae: 0.1201\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 0.0230 - mae: 0.1187\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 0.0224 - mae: 0.1172\n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 0.0219 - mae: 0.1157\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - loss: 0.0213 - mae: 0.1142\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0207 - mae: 0.11267\n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 0.0202 - mae: 0.1111\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0498 - mae: 0.1168     \n"
     ]
    }
   ],
   "source": [
    "lin_reg = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_sub_train.shape[1],)),\n",
    "    tf.keras.layers.Normalization(axis=-1), # When -1 the last axis of the input is assumed to be a feature dimension and is normalized per index.\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "  ])\n",
    "\n",
    "lin_reg.compile(loss=\"mse\", optimizer=\"adam\", metrics=['mae'])\n",
    "\n",
    "lin_reg_history = lin_reg.fit(X_sub_train, Y_sub_train, epochs=50, verbose=True)\n",
    "\n",
    "loss, mae = lin_reg.evaluate(X_validation, Y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great. Let's go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macik\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0254 - mae: 0.1310\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0012 - mae: 0.0264\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.0337e-04 - mae: 0.0158\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8229e-04 - mae: 0.0162\n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8716e-04 - mae: 0.0167\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.4767e-04 - mae: 0.0162\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.9194e-04 - mae: 0.0149\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.5792e-04 - mae: 0.0139\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.4265e-04 - mae: 0.0136\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2098e-04 - mae: 0.0133\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1146e-04 - mae: 0.0131\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2593e-04 - mae: 0.0137\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.9218e-04 - mae: 0.0130\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5123e-04 - mae: 0.0121\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.0675e-04 - mae: 0.01101\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6256e-04 - mae: 0.0098\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3932e-04 - mae: 0.0090\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2059e-04 - mae: 0.0084\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1101e-04 - mae: 0.0082  \n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1580e-04 - mae: 0.0086\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1419e-04 - mae: 0.0086\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0844e-04 - mae: 0.0084\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0362e-04 - mae: 0.0083\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0132e-04 - mae: 0.0082 \n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0042e-04 - mae: 0.0083\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0333e-04 - mae: 0.0085\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0394e-04 - mae: 0.0086 \n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0705e-04 - mae: 0.0088\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1186e-04 - mae: 0.0091 \n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0923e-04 - mae: 0.0090\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1251e-04 - mae: 0.0091\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1008e-04 - mae: 0.0091\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1344e-04 - mae: 0.0092\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0613e-04 - mae: 0.0089\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0421e-04 - mae: 0.0088\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.8750e-05 - mae: 0.0085\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.3446e-05 - mae: 0.0083\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.5363e-05 - mae: 0.0084\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.1018e-05 - mae: 0.0082\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.3928e-05 - mae: 0.0084\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.3752e-05 - mae: 0.0084\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.8430e-05 - mae: 0.0086\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.6846e-05 - mae: 0.0086\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0017e-04 - mae: 0.0087\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0495e-04 - mae: 0.0090\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0351e-04 - mae: 0.0089  \n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0901e-04 - mae: 0.0092\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0959e-04 - mae: 0.0091\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0986e-04 - mae: 0.0090 \n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1500e-04 - mae: 0.0092\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7974e-04 - mae: 0.0149 \n",
      "Mean Absolute Error: 0.01617998443543911\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_sub_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',  # Changed to sparse_categorical_crossentropy\n",
    "              metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_sub_train, Y_sub_train, epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_validation, Y_validation)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THERE we go! Now let's go very deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macik\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0018 - mae: 0.0279\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3019e-04 - mae: 0.0171\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0070e-04 - mae: 0.0146\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9874e-04 - mae: 0.0145\n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.8007e-04 - mae: 0.0141\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6557e-04 - mae: 0.0137\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.6547e-04 - mae: 0.0139\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5949e-04 - mae: 0.0135\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4115e-04 - mae: 0.0134\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4864e-04 - mae: 0.0135\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3697e-04 - mae: 0.0133\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4716e-04 - mae: 0.0133\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3275e-04 - mae: 0.0132\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5010e-04 - mae: 0.0134\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2083e-04 - mae: 0.0131\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0905e-04 - mae: 0.0128\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.0232e-04 - mae: 0.0127\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2300e-04 - mae: 0.0131\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7901e-04 - mae: 0.0122\n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7196e-04 - mae: 0.0122\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4750e-04 - mae: 0.0117\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8866e-04 - mae: 0.0105\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.4404e-04 - mae: 0.0116\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7896e-04 - mae: 0.0124\n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6388e-04 - mae: 0.0100\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3084e-04 - mae: 0.0090\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6675e-05 - mae: 0.0064\n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5953e-05 - mae: 0.0076\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2893e-04 - mae: 0.0086\n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9637e-05 - mae: 0.0060\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5040e-05 - mae: 0.0057\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.0923e-05 - mae: 0.0057\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.7330e-05 - mae: 0.0058\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0454e-05 - mae: 0.0062\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6650e-05 - mae: 0.0065\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4841e-05 - mae: 0.0059\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3797e-05 - mae: 0.0064\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6336e-05 - mae: 0.0054\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.5197e-05 - mae: 0.0059\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4206e-05 - mae: 0.0052\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.4164e-05 - mae: 0.0058\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3402e-05 - mae: 0.0051\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9389e-05 - mae: 0.0055\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5053e-05 - mae: 0.0052\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6179e-05 - mae: 0.0053\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.4947e-05 - mae: 0.0052\n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5222e-05 - mae: 0.0052\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2331e-05 - mae: 0.0050\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2300e-05 - mae: 0.0050\n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4049e-05 - mae: 0.0051\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1768e-04 - mae: 0.0110 \n",
      "Mean Absolute Error: 0.01221767533570528\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "best_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_sub_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "best_model.compile(optimizer='adam',\n",
    "              loss='mse',  # Changed to sparse_categorical_crossentropy\n",
    "              metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = best_model.fit(X_sub_train, Y_sub_train, epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = best_model.evaluate(X_validation, Y_validation)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the deeper network performed slightly better than the shallower one! Let's try something wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macik\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 10.5785 - mae: 1.9109\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1830 - mae: 0.2877\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0218 - mae: 0.1080\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0147 - mae: 0.0896\n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0086 - mae: 0.0700\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0068 - mae: 0.0631\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0081 - mae: 0.0678\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0169 - mae: 0.0963\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0293 - mae: 0.1255\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0316 - mae: 0.1337\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0252 - mae: 0.1157\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0144 - mae: 0.0903\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0054 - mae: 0.0547\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0026 - mae: 0.0398\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0026 - mae: 0.0379\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0340\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0336\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0307\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0292\n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0285\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0280\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0274\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0266\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0259\n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0011 - mae: 0.0253\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0010 - mae: 0.0247\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 9.9089e-04 - mae: 0.0243\n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 9.1751e-04 - mae: 0.0237\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 9.9573e-04 - mae: 0.0243\n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 9.6302e-04 - mae: 0.0245\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0307\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0033 - mae: 0.0431\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0081 - mae: 0.0660\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0169 - mae: 0.0975\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0380 - mae: 0.1466\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0575 - mae: 0.1837\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0460 - mae: 0.1594\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0173 - mae: 0.0969\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0071 - mae: 0.0609\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0293\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0011 - mae: 0.0238\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 8.2747e-04 - mae: 0.0211\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.4118e-04 - mae: 0.0188\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 7.2105e-04 - mae: 0.0194\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.8001e-04 - mae: 0.0191\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.7741e-04 - mae: 0.0190\n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 6.5324e-04 - mae: 0.0189\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.9283e-04 - mae: 0.0183\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.2727e-04 - mae: 0.0175\n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.8034e-04 - mae: 0.0169\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0058 - mae: 0.0306      \n",
      "Mean Absolute Error: 0.03197630122303963\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_sub_train.shape[1],)), \n",
    "    tf.keras.layers.BatchNormalization(), # Batch normalization is good with wider layers!\n",
    "    tf.keras.layers.Dense(1024, activation='relu'), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',   # Mean Squared Error for regression\n",
    "              metrics=['mae'])  # Mean Absolute Error for evaluation\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_sub_train, Y_sub_train, epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_validation, Y_validation)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like deeper is a little better for this! How about wide AND deep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macik\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - loss: 8.2688 - mae: 1.8434\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.2220 - mae: 0.3711\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 1.1574 - mae: 0.8305\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.3340 - mae: 0.4414\n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.3204 - mae: 0.4365\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.1367 - mae: 0.2672\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.0748 - mae: 0.2037\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.0247 - mae: 0.1170\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - loss: 0.0062 - mae: 0.0620\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.0037 - mae: 0.0473\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0032 - mae: 0.0450\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0033 - mae: 0.0452\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0054 - mae: 0.0573\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.0083 - mae: 0.0707\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0110 - mae: 0.0815\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0125 - mae: 0.0876\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0122 - mae: 0.0877\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.0117 - mae: 0.0863\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0138 - mae: 0.0911\n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0265 - mae: 0.1272\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0686 - mae: 0.1999\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.1600 - mae: 0.2995\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0717 - mae: 0.2126\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0467 - mae: 0.1705\n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0121 - mae: 0.0807\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.0118 - mae: 0.0796\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0108 - mae: 0.0770\n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0106 - mae: 0.0760\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0094 - mae: 0.0720\n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0101 - mae: 0.0752\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.0149 - mae: 0.0916\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0263 - mae: 0.1235\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0328 - mae: 0.1407\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0201 - mae: 0.1108\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0191 - mae: 0.1053\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0073 - mae: 0.0641\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0040 - mae: 0.0424\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.0036 - mae: 0.0405\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.0034 - mae: 0.0399\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 0.0031 - mae: 0.0385\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.0032 - mae: 0.0389\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 0.0031 - mae: 0.0394\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.0030 - mae: 0.0397\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0027 - mae: 0.0393\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0024 - mae: 0.0373\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0023 - mae: 0.0368\n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0026 - mae: 0.0380\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - loss: 0.0028 - mae: 0.0398\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 0.0028 - mae: 0.0397\n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - loss: 0.0027 - mae: 0.0385\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0177 - mae: 0.0657      \n",
      "Mean Absolute Error: 0.06904934346675873\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_sub_train.shape[1],)), \n",
    "    tf.keras.layers.BatchNormalization(), # Batch normalization is good with wider layers!\n",
    "    tf.keras.layers.Dense(1024, activation='relu'), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_sub_train.shape[1],)), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_sub_train.shape[1],)), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_sub_train.shape[1],)), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_sub_train.shape[1],)), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_sub_train.shape[1],)), \n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',   # Mean Squared Error for regression\n",
    "              metrics=['mae'])  # Mean Absolute Error for evaluation\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_sub_train, Y_sub_train, epochs=50)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_validation, Y_validation)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ehhhh, looks like deeper is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What periods of time does your model perform well in, and when does it perform poorly? Were they times of financial market stress?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macik\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0021 - mae: 0.0324\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9242e-04 - mae: 0.0161\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7058e-04 - mae: 0.0160\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.9072e-04 - mae: 0.0143\n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7259e-04 - mae: 0.0138\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7464e-04 - mae: 0.0138\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6672e-04 - mae: 0.0138\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5462e-04 - mae: 0.0136\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5412e-04 - mae: 0.0137\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4474e-04 - mae: 0.0136\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.5323e-04 - mae: 0.0137\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.3850e-04 - mae: 0.0135\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2422e-04 - mae: 0.0132\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9813e-04 - mae: 0.0128\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7244e-04 - mae: 0.0123\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5445e-04 - mae: 0.0120\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9891e-04 - mae: 0.0108\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2556e-04 - mae: 0.0136\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5509e-04 - mae: 0.0092\n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3764e-04 - mae: 0.0116\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1002e-04 - mae: 0.0080\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.2492e-05 - mae: 0.0071\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.7978e-05 - mae: 0.0068\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.5737e-05 - mae: 0.0064\n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4619e-05 - mae: 0.0062\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4758e-05 - mae: 0.0063\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5984e-05 - mae: 0.0058\n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9913e-05 - mae: 0.0060\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.3264e-05 - mae: 0.0062\n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8800e-05 - mae: 0.0054\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6921e-05 - mae: 0.0053\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.9880e-05 - mae: 0.0060\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9779e-05 - mae: 0.0055\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.1073e-05 - mae: 0.0056\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.4356e-05 - mae: 0.0063\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.7220e-05 - mae: 0.0053\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3225e-05 - mae: 0.0057\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.0104e-05 - mae: 0.0062\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6880e-05 - mae: 0.0053\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6825e-05 - mae: 0.0054\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.9632e-05 - mae: 0.0056\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3227e-05 - mae: 0.0051\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.5690e-05 - mae: 0.0053\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6788e-05 - mae: 0.0054\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1474e-05 - mae: 0.0050\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.3220e-05 - mae: 0.0051\n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5324e-05 - mae: 0.0053\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.0877e-05 - mae: 0.0049\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2033e-05 - mae: 0.0050\n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1594e-05 - mae: 0.0050\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "best_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_sub_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "best_model.compile(optimizer='adam',\n",
    "              loss='mse',  # Changed to sparse_categorical_crossentropy\n",
    "              metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = best_model.fit(X_sub_train, Y_sub_train, epochs=50)\n",
    "\n",
    "y_pred = best_model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = X_validation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Date': test_dates,  # Ensure you have corresponding dates for test set\n",
    "    'Actual': Y_validation.values.flatten(),\n",
    "    'Predicted': y_pred.flatten(),\n",
    "    'Absolute Error': np.abs(Y_validation.values.flatten() - y_pred.flatten())\n",
    "})\n",
    "\n",
    "results_df = results_df.sort_values(by='Date')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(results_df['Date'], results_df['Absolute Error'], color='red', label='Absolute Error')\n",
    "plt.title(\"Prediction Error Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Absolute Error\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model does really poorly between 2009 and 2010, and does the best between 2006 and 2007. After 2010, it looks a bit more volatile. 2009-2010 was the period of trime after the 2008 Global Financial Crisis, so it makes sense that our model doesn't predict super well during this time period. It looks like it starts performing badly around 1/4 of the way through 2009, which is when the stock market crashed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Search Tuner  \n",
    "Tuning:  \n",
    "- Optimizer  \n",
    "- Number of layers  \n",
    "- Activation functions  \n",
    "- Batch size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(64, activation=hp.Choice('activation', ['relu', 'tanh', 'sigmoid']), \n",
    "                                    input_shape=(X_sub_train.shape[1],)))\n",
    "    \n",
    "    # Tune the number of layers (between 6 and 10)\n",
    "    for i in range(hp.Int('num_layers', 6, 10)):\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            128, \n",
    "            activation=hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "        ))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))  # Regression output\n",
    "\n",
    "    # Tune optimizer\n",
    "    optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "    batch_size = hp.Choice('batch_size', [32, 64])\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',  # Regression task\n",
    "                  metrics=['mae'])  # Mean Absolute Error\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_results\\volatility_prediction\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Search Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mae',  # Minimize validation MAE\n",
    "    max_trials=30,  # Number of models to try\n",
    "    executions_per_trial=1,  # Run each model once\n",
    "    directory='tuner_results',\n",
    "    project_name='volatility_prediction'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 10s]\n",
      "val_mae: 0.014586223289370537\n",
      "\n",
      "Best val_mae So Far: 0.007832328788936138\n",
      "Total elapsed time: 00h 10m 24s\n",
      "Best Hyperparameters Found:\n",
      "Optimizer: adam\n",
      "Number of Layers: 6\n",
      "Layer 0 Activation: tanh\n",
      "Layer 1 Activation: tanh\n",
      "Layer 2 Activation: tanh\n",
      "Layer 3 Activation: tanh\n",
      "Layer 4 Activation: tanh\n",
      "Layer 5 Activation: tanh\n",
      "Batch Size: 32\n"
     ]
    }
   ],
   "source": [
    "# Perform the hyperparameter search\n",
    "tuner.search(X_sub_train, Y_sub_train,\n",
    "             validation_data = (X_validation, Y_validation),\n",
    "             epochs=50,\n",
    "             batch_size=tuner.oracle.hyperparameters.Choice('batch_size', [32, 64]))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters Found:\")\n",
    "print(f\"Optimizer: {best_hps.get('optimizer')}\")\n",
    "print(f\"Number of Layers: {best_hps.get('num_layers')}\")\n",
    "for i in range(best_hps.get('num_layers')):\n",
    "    print(f\"Layer {i} Activation: {best_hps.get('activation')}\")\n",
    "print(f\"Batch Size: {best_hps.get('batch_size')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Search, over the same hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 Complete [00h 00m 17s]\n",
      "val_mae: 0.008151134476065636\n",
      "\n",
      "Best val_mae So Far: 0.005144534166902304\n",
      "Total elapsed time: 00h 02m 16s\n",
      "Best Hyperparameters Found:\n",
      "Optimizer: nadam\n",
      "Number of Layers: 7\n",
      "Activation Function: gelu\n",
      "Batch Size: 64\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(128, activation=hp.Choice('activation', ['relu', 'tanh', 'sigmoid', 'elu', 'selu', 'gelu']), \n",
    "                                    input_shape=(X_sub_train.shape[1],)))\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 6, 10)):\n",
    "        model.add(tf.keras.layers.Dense(128, activation=hp.Choice('activation', ['relu', 'tanh', 'sigmoid', 'elu', 'selu', 'gelu'])))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(1, activation='linear'))  # Regression output\n",
    "\n",
    "    optimizer = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop', 'adamax', 'nadam'])\n",
    "\n",
    "    batch_size = hp.Choice('batch_size', [16, 32, 64])\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',  # Regression task\n",
    "                  metrics=['mae'])  # Mean Absolute Error\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "tuner1 = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_mae',  # Minimize validation MAE\n",
    "    max_trials=8,  # Number of models to try\n",
    "    executions_per_trial=1, \n",
    "    directory='tuner_results',\n",
    "    project_name='bayesian_volatility_prediction'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner1.search(X_sub_train, Y_sub_train,\n",
    "             validation_data=(X_validation, Y_validation),  # Explicit validation data\n",
    "             epochs=50,\n",
    "             batch_size=tuner.oracle.hyperparameters.Choice('batch_size', [16, 32, 64]))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner1.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"Best Hyperparameters Found:\")\n",
    "print(f\"Optimizer: {best_hps.get('optimizer')}\")\n",
    "print(f\"Number of Layers: {best_hps.get('num_layers')}\")\n",
    "print(f\"Activation Function: {best_hps.get('activation')}\")\n",
    "print(f\"Batch Size: {best_hps.get('batch_size')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# shutil.rmtree('tuner_results', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Search performed much better; it seemed to converge on a model with higher accuracy much more quickly (within 4 trials), as opposed to the Randomized Search, which didn't find this optimal method even in 30 trials. The top three models were:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Top 3 Models Found:\n",
      "\n",
      "🔹 Model 1:\n",
      "Optimizer: nadam\n",
      "Number of Layers: 7\n",
      "Activation Function: gelu\n",
      "Batch Size: 64\n",
      "\n",
      "🔹 Model 2:\n",
      "Optimizer: nadam\n",
      "Number of Layers: 10\n",
      "Activation Function: gelu\n",
      "Batch Size: 16\n",
      "\n",
      "🔹 Model 3:\n",
      "Optimizer: adam\n",
      "Number of Layers: 10\n",
      "Activation Function: elu\n",
      "Batch Size: 32\n"
     ]
    }
   ],
   "source": [
    "# Get the top 3 trials sorted by validation MAE\n",
    "top_models = tuner1.get_best_hyperparameters(num_trials=3)\n",
    "\n",
    "print(\"\\n🏆 Top 3 Models Found:\")\n",
    "\n",
    "for i, hps in enumerate(top_models):\n",
    "    print(f\"\\n🔹 Model {i+1}:\")\n",
    "    print(f\"Optimizer: {hps.get('optimizer')}\")\n",
    "    print(f\"Number of Layers: {hps.get('num_layers')}\")\n",
    "    print(f\"Activation Function: {hps.get('activation')}\")\n",
    "    print(f\"Batch Size: {hps.get('batch_size')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna do two branches: one deeper branch for more complex feature extraction, and one shallow path to capture simpler relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(X_sub_train.shape[1],))\n",
    "\n",
    "# One shared feature extraction layer\n",
    "shared = tf.keras.layers.Dense(128, activation='gelu')(inputs)\n",
    "\n",
    "# Deep branch:\n",
    "deep1 = tf.keras.layers.Dense(256, activation='gelu')(shared)\n",
    "deep2 = tf.keras.layers.Dense(256, activation='gelu')(deep1)\n",
    "deep3 = tf.keras.layers.Dense(256, activation='gelu')(deep2)\n",
    "deep4 = tf.keras.layers.Dense(256, activation='gelu')(deep3)\n",
    "deep5 = tf.keras.layers.Dense(256, activation='gelu')(deep4)\n",
    "deep6 = tf.keras.layers.Dense(256, activation='gelu')(deep5)\n",
    "deep7 = tf.keras.layers.Dense(256, activation='gelu')(deep6)\n",
    "\n",
    "# Shallow branch:\n",
    "shallow = tf.keras.layers.Dense(128, activation='gelu') (shared)\n",
    "\n",
    "# Merge branches:\n",
    "merged = tf.keras.layers.Concatenate()([deep7, shallow])\n",
    "\n",
    "# Apply a fully-connected layer after the merge:\n",
    "connect = tf.keras.layers.Dense(128, activation='gelu')(merged)\n",
    "\n",
    "# Prediction layer\n",
    "output = tf.keras.layers.Dense(1, activation='linear')(connect)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "optimizer = 'nadam'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.0318e-05 - mae: 0.0061\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1453e-05 - mae: 0.0045\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.6298e-05 - mae: 0.0057\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.3621e-05 - mae: 0.0048\n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8641e-05 - mae: 0.0043\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.0922e-05 - mae: 0.0079\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9077e-05 - mae: 0.0043\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.9544e-05 - mae: 0.0049\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2267e-05 - mae: 0.0054\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9214e-05 - mae: 0.0044\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.9863e-05 - mae: 0.0052\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1100e-05 - mae: 0.0046\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1139e-05 - mae: 0.0044\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9844e-05 - mae: 0.0060\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9759e-05 - mae: 0.0044\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5547e-05 - mae: 0.0046\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2846e-05 - mae: 0.0054\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9513e-05 - mae: 0.0044\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4634e-05 - mae: 0.0047\n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1498e-05 - mae: 0.0046\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8656e-05 - mae: 0.0042\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.6420e-05 - mae: 0.0058\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7578e-05 - mae: 0.0042\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.5127e-05 - mae: 0.0057\n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8480e-05 - mae: 0.0043\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6847e-05 - mae: 0.0049\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0504e-05 - mae: 0.0045\n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7503e-05 - mae: 0.0041\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.7666e-05 - mae: 0.0059\n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7166e-05 - mae: 0.0042\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5313e-05 - mae: 0.0046\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4.5141e-05 - mae: 0.0048\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2734e-05 - mae: 0.0048\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4234e-05 - mae: 0.0049\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.0255e-05 - mae: 0.0062\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4924e-05 - mae: 0.0050\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.4405e-05 - mae: 0.0056\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9632e-05 - mae: 0.0044\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6157e-05 - mae: 0.0050\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9690e-05 - mae: 0.0044\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.1077e-05 - mae: 0.0046\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8774e-05 - mae: 0.0043\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0587e-05 - mae: 0.0045\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8375e-05 - mae: 0.0042\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0367e-05 - mae: 0.0045\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8199e-05 - mae: 0.0042\n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0053e-05 - mae: 0.0045\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8104e-05 - mae: 0.0042\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9684e-05 - mae: 0.0044\n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.8050e-05 - mae: 0.0043\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0862e-04 - mae: 0.0067 \n",
      "Mean Absolute Error: 0.006673805415630341\n"
     ]
    }
   ],
   "source": [
    "functional_1_history = model.fit(X_sub_train, Y_sub_train, epochs=50)\n",
    "\n",
    "loss, mae = model.evaluate(X_validation, Y_validation)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanna try feeding each feature into a separate branch and then bring them ALL together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1151, 17)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_inputs = []\n",
    "feature_branches = []\n",
    "\n",
    "# Loop through 17 features:\n",
    "for i in range(16):\n",
    "    feature_input = tf.keras.layers.Input(shape=(1,))\n",
    "    feature_inputs.append(feature_input)\n",
    "\n",
    "    feature_branch = tf.keras.layers.Dense(16, activation=tf.keras.activations.gelu)(feature_input)\n",
    "    feature_branch = tf.keras.layers.Dense(16, activation=tf.keras.activations.gelu)(feature_branch)\n",
    "\n",
    "    feature_branches.append(feature_branch)\n",
    "\n",
    "merged = tf.keras.layers.Concatenate()(feature_branches)\n",
    "\n",
    "# Fully connected layers after merging\n",
    "connected = tf.keras.layers.Dense(256, activation='gelu')(merged)\n",
    "connected = tf.keras.layers.Dense(128, activation='gelu')(connected)\n",
    "connected = tf.keras.layers.Dense(64, activation='gelu')(connected)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation='linear')(connected)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=feature_inputs, outputs=output)\n",
    "optimizer = 'nadam'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy for slicing\n",
    "X_sub_train = X_sub_train.to_numpy() \n",
    "X_validation = X_validation.to_numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass each feature as a separate input array\n",
    "X_train_list = [X_sub_train[:, i].reshape(-1, 1) for i in range(16)]\n",
    "X_val_list = [X_validation[:, i].reshape(-1, 1) for i in range(16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macik\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_106', 'keras_tensor_109', 'keras_tensor_112', 'keras_tensor_115', 'keras_tensor_118', 'keras_tensor_121', 'keras_tensor_124', 'keras_tensor_127', 'keras_tensor_130', 'keras_tensor_133', 'keras_tensor_136', 'keras_tensor_139', 'keras_tensor_142', 'keras_tensor_145', 'keras_tensor_148', 'keras_tensor_151']. Received: the structure of inputs=('*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 0.0025 - mae: 0.0337\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.1916e-04 - mae: 0.0174\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4739e-04 - mae: 0.0141\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0712e-04 - mae: 0.0110\n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2567e-04 - mae: 0.0085\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.6218e-05 - mae: 0.0070\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1387e-05 - mae: 0.0065\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6115e-05 - mae: 0.0056\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1812e-05 - mae: 0.0052\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.7962e-05 - mae: 0.0050\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5324e-05 - mae: 0.0047\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.3267e-05 - mae: 0.0046\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1704e-05 - mae: 0.0044\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0497e-05 - mae: 0.0043\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9559e-05 - mae: 0.0042\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8820e-05 - mae: 0.0041\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.8230e-05 - mae: 0.0041\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7752e-05 - mae: 0.0040\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7358e-05 - mae: 0.0040\n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7030e-05 - mae: 0.0040\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6752e-05 - mae: 0.0039\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6514e-05 - mae: 0.0039\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6308e-05 - mae: 0.0039\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6127e-05 - mae: 0.0039\n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5967e-05 - mae: 0.0038\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5825e-05 - mae: 0.0038\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5697e-05 - mae: 0.0038\n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5581e-05 - mae: 0.0038\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5476e-05 - mae: 0.0038\n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5379e-05 - mae: 0.0038\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5291e-05 - mae: 0.0038\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5208e-05 - mae: 0.0038\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5132e-05 - mae: 0.0038\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5061e-05 - mae: 0.0038\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4994e-05 - mae: 0.0038\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4931e-05 - mae: 0.0038\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4872e-05 - mae: 0.0037\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4817e-05 - mae: 0.0037\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4764e-05 - mae: 0.0037\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4714e-05 - mae: 0.0037\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4666e-05 - mae: 0.0037\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4621e-05 - mae: 0.0037\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.4588e-05 - mae: 0.0037\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4536e-05 - mae: 0.0037\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4497e-05 - mae: 0.0037\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4459e-05 - mae: 0.0037\n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4423e-05 - mae: 0.0037\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4389e-05 - mae: 0.0037\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4356e-05 - mae: 0.0037\n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4325e-05 - mae: 0.0037\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_list, Y_sub_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\macik\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_106', 'keras_tensor_109', 'keras_tensor_112', 'keras_tensor_115', 'keras_tensor_118', 'keras_tensor_121', 'keras_tensor_124', 'keras_tensor_127', 'keras_tensor_130', 'keras_tensor_133', 'keras_tensor_136', 'keras_tensor_139', 'keras_tensor_142', 'keras_tensor_145', 'keras_tensor_148', 'keras_tensor_151']. Received: the structure of inputs=('*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6403e-05 - mae: 0.00251\n",
      "Mean Absolute Error: 0.002911592600867152\n"
     ]
    }
   ],
   "source": [
    "loss, mae = model.evaluate(X_val_list, Y_validation)\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! Separating each feature into its own branch worked the best by far. That's really cool. I wonder if we could categorize features into different categories and pass each of those to its own branch (price info, news info, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
